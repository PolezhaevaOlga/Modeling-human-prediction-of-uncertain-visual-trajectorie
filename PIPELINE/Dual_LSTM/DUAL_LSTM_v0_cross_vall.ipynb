{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a996486-9609-42bc-aa48-0c81cf859522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import IPython\n",
    "import IPython.display\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import rcParams\n",
    "from keras.models import Sequential\n",
    "import scipy\n",
    "from scipy import io\n",
    "from scipy.interpolate import interp1d\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score,mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Flatten, Dense, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66cb24f-8db3-4205-a9e6-35c67b4e0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ok save mean and std, but min max\n",
    "def interpolate_signal(insignal, len1, len2):\n",
    "\n",
    "    first_point = insignal[0]\n",
    "    last_point = insignal[-1]\n",
    "\n",
    "    x1 = np.linspace(0, len2 - 1, len1)\n",
    "    f = interp1d(x1, insignal, axis=0, kind='linear')\n",
    "    x2 = np.linspace(0, len2 - 1, len2)\n",
    "    interpolated = f(x2)\n",
    "\n",
    "    interpolated[0] = first_point\n",
    "    interpolated[-1] = last_point\n",
    "    \n",
    "    return interpolated\n",
    "\n",
    "def list_to_interpolate_preserving_stats(input_list, output_size=145):\n",
    "    inter_list = []\n",
    "    for elem in input_list:\n",
    "        original_mean = np.nanmean(elem)\n",
    "        original_std = np.nanstd(elem)\n",
    "\n",
    "        np_int = interpolate_signal(elem, elem.shape[0], output_size)\n",
    "\n",
    "        central_part = np_int[1:-1]\n",
    "\n",
    "        interpolated_mean = np.nanmean(central_part)\n",
    "        interpolated_std = np.nanstd(central_part)\n",
    "\n",
    "        scaled_central = (central_part - interpolated_mean) * (original_std / interpolated_std) + original_mean\n",
    "\n",
    "        scaled_int = np.copy(np_int)\n",
    "        scaled_int[1:-1] = scaled_central\n",
    "\n",
    "        total_mean = np.nanmean(scaled_int)\n",
    "        total_std = np.nanstd(scaled_int)\n",
    "        scaled_int = (scaled_int - total_mean) * (original_std / total_std) + original_mean\n",
    "\n",
    "        scaled_int[0] = elem[0]\n",
    "        scaled_int[-1] = elem[-1]\n",
    "        inter_list.append(scaled_int)\n",
    "    \n",
    "    return inter_list\n",
    "\n",
    "def scaled_data2(input_list):\n",
    "\n",
    "    global_min = np.nanmin([np.nanmin(traj) for traj in input_list])\n",
    "    global_max = np.nanmax([np.nanmax(traj) for traj in input_list])\n",
    "\n",
    "    if global_max == global_min:\n",
    "        raise ValueError(\"All trajectories have the same constant value; scaling is not possible.\")\n",
    "    \n",
    "    normalized_list = []\n",
    "\n",
    "    for np_array in input_list:\n",
    "        output_array = 2 * ((np_array - global_min) / (global_max - global_min)) - 1\n",
    "\n",
    "        #output_array -= output_array[0]\n",
    "\n",
    "        #range_factor = max(1, np.max(np.abs(output_array)))\n",
    "        #output_array /= range_factor\n",
    "\n",
    "        normalized_list.append(output_array)\n",
    "\n",
    "    return normalized_list, global_max, global_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed40a90-4e35-45f2-be9b-07cc566b7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Load data\n",
    "    data = scipy.io.loadmat('../data/trajectories.mat')\n",
    "    \n",
    "    # Path\n",
    "    iid = data['iidind'][2:,]\n",
    "    rdw = data['rdwind'][2:,]\n",
    "    posr = np.zeros((iid.shape[0] - 1, iid.shape[1]))\n",
    "    posw = np.zeros((rdw.shape[0]-1, rdw.shape[1]))\n",
    "    n_path = posr.shape[1]\n",
    "    invis = []\n",
    "    normativ_label_iid = np.zeros((1, n_path))  # ideal responses\n",
    "    normativ_label_rdw = np.zeros((1, n_path))  # ideal responses\n",
    "    for tidx in range(n_path):\n",
    "        # IID\n",
    "        invisR = int(iid[0, tidx])\n",
    "        invis.append(invisR)\n",
    "        r = iid[1:, tidx].copy()\n",
    "        #r[-invisR + 1:] = np.nan\n",
    "        r[-invisR + 1:] = 0\n",
    "        posr[:, tidx] = r\n",
    "        normativ_label_iid[0, tidx] = 1 if np.nanmean(posr[:, tidx]) < 0 else 0\n",
    "        # rdw \n",
    "        w = rdw[1:, tidx].copy()\n",
    "        #w[-invisR+1:] = np.nan\n",
    "        w[-invisR+1:] = 0\n",
    "        posw[:, tidx] = w\n",
    "        normativ_label_rdw[0, tidx] = 1 if posw[-invis[tidx], tidx] < 0 else 0\n",
    "    \n",
    "    # # Create a list of columns from posr without NaN values\n",
    "    # path_iid = [np.array(col[~np.isnan(col)]) for col in posr.T]\n",
    "    \n",
    "    # # Create a list of columns from posw without NaN values\n",
    "    # path_rdw = [np.array(col[~np.isnan(col)]) for col in posw.T]\n",
    "    normativ_label_rdw =normativ_label_rdw.reshape(200,)\n",
    "    normativ_label_iid =normativ_label_iid.reshape(200,)\n",
    "    \n",
    "    ### rdw ###\n",
    "    normativ_label_rdw_same = np.hstack((normativ_label_rdw[0:50], normativ_label_rdw[150:]))\n",
    "    normativ_label_rdw_op = normativ_label_rdw[50:150]\n",
    "    normativ_list_label_rdw_same = [np.array(col[~np.isnan(col)]) for col in normativ_label_rdw_same]\n",
    "    normativ_list_label_rdw_op = [np.array(col[~np.isnan(col)]) for col in normativ_label_rdw_op]\n",
    "    normativ_labels_r = np.concatenate((normativ_list_label_rdw_same, normativ_list_label_rdw_op ), axis=0)\n",
    "\n",
    "    posw_same = np.hstack((posw[:,0:50], posw[:,150:]))\n",
    "    posw_opposite = posw[:,50:150]\n",
    "    # Create a list of columns from posr without NaN values\n",
    "    path_rdw_s = [np.array(col[~np.isnan(col)]) for col in posw_same.T]\n",
    "    path_rdw_op =[np.array(col[~np.isnan(col)]) for col in posw_opposite.T]\n",
    "\n",
    "    ### iid ###\n",
    "    normativ_label_iid_same = np.hstack((normativ_label_iid[0:50], normativ_label_iid[150:]))\n",
    "    normativ_label_iid_op = normativ_label_iid[50:150]\n",
    "    normativ_list_label_iid_same = [np.array(col[~np.isnan(col)]) for col in normativ_label_iid_same]\n",
    "    normativ_list_label_iid_op = [np.array(col[~np.isnan(col)]) for col in normativ_label_iid_op]\n",
    "    normativ_labels_i = np.concatenate((normativ_list_label_iid_same, normativ_list_label_iid_op ), axis=0)\n",
    "\n",
    "    posr_same = np.hstack((posr[:,0:50], posr[:,150:]))\n",
    "    posr_opposite = posr[:,50:150]\n",
    "    # Create a list of columns from posr without NaN values\n",
    "    path_iid_s = [np.array(col[~np.isnan(col)]) for col in posr_same.T]\n",
    "    path_iid_op =[np.array(col[~np.isnan(col)]) for col in posr_opposite.T]    \n",
    "    \n",
    "    return path_iid_s, path_iid_op, path_rdw_s, path_rdw_op\n",
    "\n",
    "\n",
    "def interpolate_signal(insignal, len1, len2):\n",
    "\n",
    "    first_point = insignal[0]\n",
    "    last_point = insignal[-1]\n",
    "\n",
    "    x1 = np.linspace(0, len2 - 1, len1)\n",
    "    f = interp1d(x1, insignal, axis=0, kind='linear')\n",
    "    x2 = np.linspace(0, len2 - 1, len2)\n",
    "    interpolated = f(x2)\n",
    "\n",
    "    interpolated[0] = first_point\n",
    "    interpolated[-1] = last_point\n",
    "    \n",
    "    return interpolated\n",
    "\n",
    "def list_to_interpolate_preserving_stats(input_list, output_size=145):\n",
    "    inter_list = []\n",
    "    for elem in input_list:\n",
    "        np_int = interpolate_signal(elem, elem.shape[0], output_size)\n",
    "        inter_list.append(np_int)\n",
    "    return inter_list\n",
    "\n",
    "def scaled_data2(input_list):\n",
    "\n",
    "    global_min = np.nanmin([np.nanmin(traj) for traj in input_list])\n",
    "    global_max = np.nanmax([np.nanmax(traj) for traj in input_list])\n",
    "\n",
    "    if global_max == global_min:\n",
    "        raise ValueError(\"All trajectories have the same constant value; scaling is not possible.\")\n",
    "    \n",
    "    normalized_list = []\n",
    "\n",
    "    for np_array in input_list:\n",
    "        output_array = 2 * ((np_array - global_min) / (global_max - global_min)) - 1\n",
    "\n",
    "        #output_array -= output_array[0]\n",
    "\n",
    "        #range_factor = max(1, np.max(np.abs(output_array)))\n",
    "        #output_array /= range_factor\n",
    "\n",
    "        normalized_list.append(output_array)\n",
    "\n",
    "    return normalized_list, global_max, global_min\n",
    "\n",
    "def load_responses(resp_array):\n",
    "    resp_same =  np.vstack((resp_array[0:50:,], resp_array[150::,]))\n",
    "    resp_op = resp_array[50:150:,]\n",
    "    return resp_same,resp_op\n",
    "\n",
    "\n",
    "def shuffle_data(trajectories, responses,rt):\n",
    "    indices = np.arange(len(trajectories))  \n",
    "    np.random.seed(15)\n",
    "    np.random.shuffle(indices)  \n",
    "    \n",
    "    shuffled_trajectories = [trajectories[i] for i in indices]  \n",
    "    shuffled_responses = responses[indices,: ]  \n",
    "    shuffled_rt = rt[indices,: ]      \n",
    "    return shuffled_trajectories, shuffled_responses,shuffled_rt \n",
    "\n",
    "def make_dataset (path_iid_s,path_iid_op, resp_iid_s,resp_iid_op,rt_iid_s,rt_iid_op):\n",
    "    feature_list =[]\n",
    "    label_list = []\n",
    "    rt_list=[]\n",
    "    for i in range(resp_iid_s.shape[1]):\n",
    "        resp_one_subj_iid_s = resp_iid_s[:, i]\n",
    "        resp_one_subj_iid_op = resp_iid_op[:, i]\n",
    "        \n",
    "        rt_one_subj_iid_s = rt_iid_s[:, i]\n",
    "        rt_one_subj_iid_op = rt_iid_op[:, i]  \n",
    "        \n",
    "        resp_list_iid_s = [np.array(col[~np.isnan(col)]) for col in resp_one_subj_iid_s]\n",
    "        resp_list_iid_op = [np.array(col[~np.isnan(col)]) for col in resp_one_subj_iid_op]\n",
    "\n",
    "        rt_list_iid_s = [col[col != 0] for col in rt_one_subj_iid_s]\n",
    "        rt_list_iid_op = [col[col != 0] for col in rt_one_subj_iid_op]\n",
    "\n",
    "        scaled_data = np.concatenate((path_iid_s, path_iid_op), axis=0)\n",
    "        labels = np.concatenate((resp_list_iid_s, resp_list_iid_op), axis=0)\n",
    "       \n",
    "        rts = np.concatenate((rt_list_iid_s, rt_list_iid_op), axis=0)\n",
    "\n",
    "        feature_list.append(scaled_data)\n",
    "        label_list.append(labels)\n",
    "        rt_list.append(rts)\n",
    "    return feature_list, label_list,rt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90382b99-41d6-4aa2-924d-0b0f8ac90534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(feature_list, label_list,rt_list, condition):\n",
    "    combined_data = list(zip(feature_list, label_list, rt_list)) # Combine features and labels into a single list for shuffling\n",
    "    np.random.seed(10)\n",
    "    np.random.shuffle(combined_data) # Shuffle the combined data\n",
    "    feature_list, label_list, rt_list = zip(*combined_data) # Unpack the shuffled data back into features and labels\n",
    "    feature_list = list(feature_list) # Convert the shuffled lists back to regular Python lists\n",
    "    label_list = list(label_list)\n",
    "    rt_list = list(rt_list)\n",
    "    \n",
    "    norm_list= []\n",
    "    # norm rdw \n",
    "    if condition==\"rdw\":\n",
    "        norm_list.extend([np.where(feature_list[0][:,-1] > 0, 0, 1)] * 28)    \n",
    "    elif condition==\"iid\":\n",
    "    #norm iid \n",
    "        norm_list.extend([np.where(np.mean(feature_list[0].T, axis=0) > 0, 0, 1)] * 28)\n",
    "    \n",
    "    X = np.concatenate([f.reshape(f.shape[0], -1) for f in feature_list], axis=0)    \n",
    "    y = np.concatenate(label_list, axis=0)\n",
    "    rt = np.concatenate(rt_list, axis=0)\n",
    "    \n",
    "    norm = np.array(norm_list)\n",
    "    all_y = np.concatenate([y[i].reshape(-1) for i in range(len(y))], axis=0)\n",
    "    all_rt = np.concatenate([rt[i].reshape(-1) for i in range(len(rt))], axis=0)\n",
    "    all_norm = np.concatenate([norm[i].reshape(-1) for i in range(len(norm))], axis=0)\n",
    "    # X.shape, y.shape,all_y.shape, all_norm.shape\n",
    "    return X,all_y,all_rt,all_norm\n",
    "\n",
    "\n",
    "def remove_nan_indices(X, y, rt):\n",
    "   \n",
    "    #X = np.array(X)\n",
    "    #y = np.array(y)\n",
    "    #rt = np.array(rt)\n",
    "\n",
    "    valid_indices = ~np.isnan(rt)\n",
    "    X_clean = X[valid_indices]\n",
    "    y_clean = y[valid_indices]\n",
    "    rt_clean = rt[valid_indices]\n",
    "\n",
    "    return X_clean, y_clean, rt_clean\n",
    "\n",
    "def create_recent_points_data(X, y, rt, window_size):\n",
    "    X_windows = []\n",
    "    y_windows = []\n",
    "    rt_windows = []\n",
    "    \n",
    "    for i in range(X.shape[0]):  \n",
    "        trajectory = X[i]\n",
    "        label = y[i]\n",
    "        rt_value = rt[i]\n",
    "        invis = len(np.where(trajectory == 0)[0])-1\n",
    "        trajectory_non_zero = trajectory[:-invis]\n",
    "        # last point in window_size \n",
    "        window = trajectory_non_zero[-window_size:]\n",
    "        X_windows.append(window)\n",
    "        y_windows.append(label)\n",
    "        rt_windows.append(rt_value)\n",
    "    \n",
    "    return np.array(X_windows), np.array(y_windows), np.array(rt_windows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08137093-b735-48fe-b75b-88e69d44e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_iid_s, path_iid_op, path_rdw_s, path_rdw_op = load_data()\n",
    "\n",
    "\n",
    "rt_rdw = pd.read_csv('../data/pros_rt_rdw.csv', header=None).to_numpy()\n",
    "rt_iid = pd.read_csv('../data/pros_rt_iid.csv', header=None).to_numpy()\n",
    "resp_rdw = pd.read_csv('../data/resp_rdw.csv', header=None).to_numpy()\n",
    "resp_iid = pd.read_csv('../data/resp_iid.csv', header=None).to_numpy()\n",
    "\n",
    "acc_rdw = pd.read_csv('../data/acc_rdw.csv', header=None).to_numpy()\n",
    "acc_iid = pd.read_csv('../data/acc_iid.csv', header=None).to_numpy()\n",
    "\n",
    "resp_iid_same,resp_iid_op= load_responses(resp_iid)\n",
    "resp_rdw_same,resp_rdw_op= load_responses(resp_rdw)\n",
    "\n",
    "rt_iid_same,rt_iid_op= load_responses(rt_iid)\n",
    "rt_rdw_same,rt_rdw_op= load_responses(rt_rdw)\n",
    "\n",
    "norm_acc_iid_same,norm_acc_iid_op = load_responses(acc_iid)\n",
    "norm_acc_rdw_same,norm_acc_rdw_op = load_responses(acc_rdw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c9d6fe-f69d-4c1d-9550-bdeb652a9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### IID ##########\n",
    "##interpolation \n",
    "path_iid_same_interp_list = list_to_interpolate_preserving_stats(path_iid_s, output_size=150)\n",
    "path_iid_op_interp_list = list_to_interpolate_preserving_stats(path_iid_op, output_size=150)\n",
    "# Normalization\n",
    "scaled_list_iid_same, _, _,  = scaled_data2(path_iid_same_interp_list)  # Transpose for correct shape\n",
    "scaled_list_iid_op, _, _,  = scaled_data2(path_iid_op_interp_list)  # Transpose for correct shape\n",
    "\n",
    "######### RDW ##########\n",
    "##interpolation \n",
    "path_rdw_same_interp_list = list_to_interpolate_preserving_stats(path_rdw_s, output_size=150)\n",
    "path_rdw_op_interp_list = list_to_interpolate_preserving_stats(path_rdw_op, output_size=150)\n",
    "## Normalization\n",
    "scaled_list_rdw_same, _, _,  = scaled_data2(path_rdw_same_interp_list)  # Transpose for correct shape\n",
    "scaled_list_rdw_op, _, _,  = scaled_data2(path_rdw_op_interp_list)  # Transpose for correct shape\n",
    "\n",
    "\n",
    "scaled_list_iid_same, resp_iid_same,rt_iid_same = shuffle_data(scaled_list_iid_same, resp_iid_same,rt_iid_same)\n",
    "scaled_list_iid_op, resp_iid_op, rt_iid_op = shuffle_data(scaled_list_iid_op, resp_iid_op,rt_iid_op)\n",
    "\n",
    "scaled_list_rdw_same, resp_rdw_same,rt_rdw_same = shuffle_data(scaled_list_rdw_same, resp_rdw_same,rt_rdw_same)\n",
    "scaled_list_rdw_op, resp_rdw_op,rt_rdw_op = shuffle_data(scaled_list_rdw_op, resp_rdw_op,rt_rdw_op)\n",
    "\n",
    "feature_list_iid, label_list_iid,rt_list_iid = make_dataset(scaled_list_iid_same,scaled_list_iid_op, \n",
    "                                                        resp_iid_same,resp_iid_op,rt_iid_same, rt_iid_op)\n",
    "\n",
    "feature_list_rdw, label_list_rdw,rt_list_rdw = make_dataset (scaled_list_rdw_same,scaled_list_rdw_op,\n",
    "                                                             resp_rdw_same,resp_rdw_op,rt_rdw_same, rt_rdw_op)\n",
    "\n",
    "\n",
    "X_iid, all_y_iid, all_rt_iid,all_norm_iid = shuffle_dataset(feature_list_iid, label_list_iid,rt_list_iid,\"iid\")\n",
    "X_rdw, all_y_rdw,all_rt_rdw, all_norm_rdw =shuffle_dataset(feature_list_rdw, label_list_rdw,rt_list_rdw,\"rdw\")\n",
    "\n",
    "X_iid, all_y_iid, all_rt_iid = remove_nan_indices(X_iid, all_y_iid, all_rt_iid)\n",
    "X_rdw, all_y_rdw, all_rt_rdw = remove_nan_indices(X_rdw, all_y_rdw, all_rt_rdw)\n",
    "\n",
    "X_iid_reshaped = X_iid.reshape(X_iid.shape[0], X_iid.shape[1], 1)\n",
    "X_rdw_reshaped = X_rdw.reshape(X_rdw.shape[0], X_rdw.shape[1], 1)\n",
    "\n",
    "window_size = 10 \n",
    "\n",
    "\n",
    "X_iid_recent, y_iid_recent, rt_iid_recent = create_recent_points_data(X_iid_reshaped, all_y_iid, all_rt_iid, window_size)\n",
    "X_rdw_recent, y_rdw_recent, rt_rdw_recent = create_recent_points_data(X_rdw_reshaped, all_y_rdw, all_rt_rdw, window_size)\n",
    "\n",
    "print(\"Shape of X_iid_recent:\", X_iid_recent.shape)  #  (path*nsubj, window_size, 1)\n",
    "print(\"Shape of y_iid_recent:\", y_iid_recent.shape)  # path*nsubj: (5600,)\n",
    "print(\"Shape of rt_iid_recent:\", rt_iid_recent.shape)  # path*nsubj: (5600,)\n",
    "\n",
    "np.random.seed(42)\n",
    "X_combined_resent = np.concatenate((X_iid_recent, X_rdw_recent), axis=0)\n",
    "y_combined_resent = np.concatenate((y_iid_recent, y_rdw_recent), axis=0)\n",
    "rt_combined = np.concatenate((rt_iid_recent, rt_rdw_recent), axis=0)\n",
    "\n",
    "X_combined_full = np.concatenate((X_iid_reshaped, X_rdw_reshaped), axis=0)\n",
    "\n",
    "X_full_train_comb, X_full_val_comb, X_recent_train_comb, X_recent_val_comb, y_binary_train_comb, y_binary_val_comb, y_time_train_comb, y_time_val_comb = train_test_split(\n",
    "X_combined_full, X_combined_resent, y_combined_resent, rt_combined, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "print(f'Training set size: {X_full_train_comb.shape}, {X_recent_train_comb.shape}, {y_binary_train_comb.shape}, {y_time_train_comb.shape}')\n",
    "print(f'Validation set size: {X_full_val_comb.shape}, {X_recent_val_comb.shape}, {y_binary_val_comb.shape}, {y_time_val_comb.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acace0ce-2b32-4f38-994f-041153d4d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_full_train_iid, X_full_val_iid, X_recent_train_iid, X_recent_val_iid, y_binary_train_iid, y_binary_val_iid, y_time_train_iid, y_time_val_iid = train_test_split(\n",
    "X_iid_reshaped, X_iid_recent, y_iid_recent, rt_iid_recent, test_size=0.1, random_state=40)\n",
    "\n",
    "\n",
    "print(f'Training set size: {X_full_train_iid.shape}, {X_recent_train_iid.shape}, {y_binary_train_iid.shape}, {y_time_train_iid.shape}')\n",
    "print(f'Validation set size: {X_full_val_iid.shape}, {X_recent_val_iid.shape}, {y_binary_val_iid.shape}, {y_time_val_iid.shape}')\n",
    "\n",
    "X_full_train_rdw, X_full_val_rdw, X_recent_train_rdw, X_recent_val_rdw, y_binary_train_rdw, y_binary_val_rdw, y_time_train_rdw, y_time_val_rdw = train_test_split(\n",
    "X_rdw_reshaped, X_rdw_recent, y_rdw_recent, rt_rdw_recent, test_size=0.1, random_state=40)\n",
    "\n",
    "\n",
    "print(f'Training set size: {X_full_train_rdw.shape}, {X_recent_train_rdw.shape}, {y_binary_train_rdw.shape}, {y_time_train_rdw.shape}')\n",
    "print(f'Validation set size: {X_full_val_rdw.shape}, {X_recent_val_rdw.shape}, {y_binary_val_rdw.shape}, {y_time_val_rdw.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d16082-a5d6-4ad2-9bf1-2bc88119e154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a913e32-073a-44a8-b2ec-f5590c0de8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_full_trajectory, X_recent_trajectory, y_true_binary, y_true_time):\n",
    "    predictions = model.predict({'full_trajectory': X_full_trajectory, 'recent_trajectory': X_recent_trajectory})\n",
    "\n",
    "    binary_predictions = predictions[0]\n",
    "    time_predictions = predictions[1]\n",
    "\n",
    "    binary_predictions = (binary_predictions > 0.5).astype(int)\n",
    "\n",
    "    binary_accuracy = accuracy_score(y_true_binary, binary_predictions)\n",
    "    precision = precision_score(y_true_binary, binary_predictions)\n",
    "    recall = recall_score(y_true_binary, binary_predictions)\n",
    "    f1 = f1_score(y_true_binary, binary_predictions)\n",
    "    conf_matrix = confusion_matrix(y_true_binary, binary_predictions)\n",
    "\n",
    "    conf_matrix_percent = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "    rmse_time = np.sqrt(mean_squared_error(y_true_time, time_predictions))\n",
    "    rt_range = np.max(y_true_time) - np.min(y_true_time)\n",
    "    relative_rmse_time = rmse_time / rt_range\n",
    "    #relative_rmse_time = rmse_time / (all_rt_iid.max() - all_rt_iid.min())\n",
    "\n",
    "    metrics = {\n",
    "        'Binary Accuracy': binary_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Confusion Matrix (%)': conf_matrix_percent,\n",
    "        'RMSE for RT Predictions': rmse_time,\n",
    "        'Relative RMSE for RT Predictions': relative_rmse_time\n",
    "    }\n",
    "\n",
    "    return metrics, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e714c72e-ab6d-42d3-a387-b065f6959e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(model, X_full_trajectory, X_recent_trajectory, y_true_binary, y_true_time):\n",
    "\n",
    "    predictions = model.predict({'full_trajectory': X_full_trajectory, 'recent_trajectory': X_recent_trajectory})\n",
    "\n",
    "    binary_predictions = predictions[0]  # Binary response predictions\n",
    "    time_predictions = predictions[1]    # Reaction time predictions\n",
    "\n",
    "    binary_predictions = (binary_predictions > 0.5).astype(int)\n",
    "\n",
    "    binary_accuracy = accuracy_score(y_true_binary, binary_predictions)\n",
    "    precision = precision_score(y_true_binary, binary_predictions)\n",
    "    recall = recall_score(y_true_binary, binary_predictions)\n",
    "    f1 = f1_score(y_true_binary, binary_predictions)\n",
    "    conf_matrix = confusion_matrix(y_true_binary, binary_predictions)\n",
    "\n",
    "\n",
    "    rmse_time = np.sqrt(mean_squared_error(y_true_time, time_predictions))\n",
    "    relative_rmse_time = rmse_time / (y_true_time.max() - y_true_time.min())\n",
    "    metrics = {\n",
    "        'Binary Accuracy': binary_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'RMSE for RT Predictions': rmse_time,\n",
    "        'Relative RMSE for RT Predictions': relative_rmse_time\n",
    "    }\n",
    "\n",
    "    return metrics,predictions\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "\n",
    "    rcParams.update({'font.size': 20})  \n",
    "    rcParams['font.family'] = 'sans-serif'\n",
    "    rcParams['font.sans-serif'] = ['Calibri']  \n",
    "\n",
    "    plt.figure(figsize=(27, 9))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['response_accuracy'])\n",
    "    plt.plot(history.history['val_response_accuracy'])\n",
    "    plt.title('Model Accuracy (Response)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history.history['reaction_time_mse'])\n",
    "    plt.plot(history.history['val_reaction_time_mse'])\n",
    "    plt.title('Mean Squared Error (RT)')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ffe029-0a18-44ed-aaf3-36b20e11f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, X_full_train, X_recent_train, y_binary_train, y_time_train, \n",
    "                    X_full_val, X_recent_val, y_binary_val, y_time_val, patience=40):\n",
    " \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "    \n",
    "  \n",
    "    history = model.fit(\n",
    "        {'full_trajectory': X_full_train, 'recent_trajectory': X_recent_train}, \n",
    "        {'response': y_binary_train, 'reaction_time': y_time_train},  \n",
    "        epochs=100, batch_size=64, \n",
    "        validation_data=(\n",
    "            {'full_trajectory': X_full_val, 'recent_trajectory': X_recent_val},  \n",
    "            {'response': y_binary_val, 'reaction_time': y_time_val} \n",
    "        ),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f2eed-b7b3-4f73-8ac4-13272d350318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut,KFold\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_model():\n",
    "    full_input = tf.keras.Input(shape=(150, 1), name='full_trajectory')\n",
    "    recent_input = tf.keras.Input(shape=(10, 1), name='recent_trajectory')\n",
    "    recent_lstm = tf.keras.layers.LSTM(64, activation='relu', return_sequences=False)(recent_input)\n",
    "    recent_lstm = tf.keras.layers.Dropout(0.1)(recent_lstm)\n",
    "    full_flattened = tf.keras.layers.Flatten()(full_input)\n",
    "    dense_layer = tf.keras.layers.Dense(64, activation='relu')(full_flattened)\n",
    "    time_output = tf.keras.layers.Dense(1, name='reaction_time', activation='linear')(dense_layer)\n",
    "    response_output = tf.keras.layers.Dense(1, activation='sigmoid', name='response')(recent_lstm)\n",
    "    model = tf.keras.Model(inputs=[full_input, recent_input], outputs=[response_output, time_output])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                  loss={'response': 'binary_crossentropy', 'reaction_time': 'mse'},\n",
    "                  metrics={'response': 'accuracy', 'reaction_time': 'mse'})\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ebbca9-d054-44d5-9892-705ac14354e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_combined = np.concatenate([X_iid_reshaped, X_rdw_reshaped], axis=0)\n",
    "X_recent_combined = np.concatenate([X_iid_recent, X_rdw_recent], axis=0)\n",
    "y_binary_combined = np.concatenate([y_iid_recent, y_rdw_recent], axis=0)\n",
    "y_time_combined = np.concatenate([rt_iid_recent, rt_rdw_recent], axis=0)\n",
    "path_type_combined = np.array(['iid'] * len(X_iid_recent) + ['rdw'] * len(X_rdw_recent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac339c-d60d-4bee-aca6-8df06ae8a8ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iid = len(X_iid_reshaped)\n",
    "n_rdw = len(X_rdw_reshaped)\n",
    "\n",
    "indices_iid = np.random.permutation(n_iid)\n",
    "indices_rdw = np.random.permutation(n_rdw)\n",
    "\n",
    "n_folds = min(n_iid, n_rdw) // (batch_size // 2)  \n",
    "\n",
    "iid_folds = np.array_split(indices_iid, n_folds)\n",
    "rdw_folds = np.array_split(indices_rdw, n_folds)\n",
    "all_metrics_by_fold = []\n",
    "fold_histories = []\n",
    "for fold in trange(n_folds, desc=\"LOO Fold Progress\"):\n",
    "\n",
    "    val_idx_iid = iid_folds[fold]\n",
    "    val_idx_rdw = rdw_folds[fold]\n",
    "\n",
    "\n",
    "    train_idx_iid = np.setdiff1d(indices_iid, val_idx_iid)\n",
    "    train_idx_rdw = np.setdiff1d(indices_rdw, val_idx_rdw)\n",
    "\n",
    "    X_full_train = np.concatenate([X_iid_reshaped[train_idx_iid], X_rdw_reshaped[train_idx_rdw]])\n",
    "    X_recent_train = np.concatenate([X_iid_recent[train_idx_iid], X_rdw_recent[train_idx_rdw]])\n",
    "    y_binary_train = np.concatenate([y_iid_recent[train_idx_iid], y_rdw_recent[train_idx_rdw]])\n",
    "    y_time_train = np.concatenate([rt_iid_recent[train_idx_iid], rt_rdw_recent[train_idx_rdw]])\n",
    "\n",
    "    X_full_val = np.concatenate([X_iid_reshaped[val_idx_iid], X_rdw_reshaped[val_idx_rdw]])\n",
    "    X_recent_val = np.concatenate([X_iid_recent[val_idx_iid], X_rdw_recent[val_idx_rdw]])\n",
    "    y_binary_val = np.concatenate([y_iid_recent[val_idx_iid], y_rdw_recent[val_idx_rdw]])\n",
    "    y_time_val = np.concatenate([rt_iid_recent[val_idx_iid], rt_rdw_recent[val_idx_rdw]])\n",
    "    path_type_val = np.array(['iid'] * len(val_idx_iid) + ['rdw'] * len(val_idx_rdw))\n",
    "\n",
    "    model = create_model()\n",
    "    history = model.fit(\n",
    "        {'full_trajectory': X_full_train, 'recent_trajectory': X_recent_train},\n",
    "        {'response': y_binary_train, 'reaction_time': y_time_train},\n",
    "        validation_data=(\n",
    "        {'full_trajectory': X_full_val, 'recent_trajectory': X_recent_val},\n",
    "        {'response': y_binary_val, 'reaction_time': y_time_val}\n",
    "    ),\n",
    "        batch_size=64, epochs=100, verbose=0\n",
    "    )\n",
    "    fold_histories.append({\n",
    "        'Fold': fold,\n",
    "        'History': history.history\n",
    "    })\n",
    "    \n",
    "    for path_type in ['iid', 'rdw']:\n",
    "        mask = (path_type_val == path_type)\n",
    "        if np.any(mask):\n",
    "            sub_metrics, _ = evaluate_model(\n",
    "                model,\n",
    "                X_full_val[mask],\n",
    "                X_recent_val[mask],\n",
    "                y_binary_val[mask],\n",
    "                y_time_val[mask]\n",
    "            )\n",
    "            sub_metrics['Fold'] = fold\n",
    "            sub_metrics['Path Type'] = path_type\n",
    "\n",
    "            all_metrics_by_fold.append(sub_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a27176-794e-49c5-9174-0c15514908c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(record['History'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42dfd9c-d5b0-41fb-a976-ce448c127f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_combined = load_model('DUAL_LSTM_MODEL.keras')\n",
    "model_combined_rdw = load_model('DUAL_LSTM_MODEL.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb8805f-940b-4b4e-9f8a-5860bb427649",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = model_combined_full.evaluate(\n",
    "    {'full_trajectory': X_full_train_comb, 'recent_trajectory': X_recent_train_comb},  \n",
    "    {'response': y_binary_train_comb, 'reaction_time': y_time_train_comb}, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "val_results = model_combined_full.evaluate(\n",
    "    {'full_trajectory': X_full_val_comb, 'recent_trajectory': X_recent_val_comb},  \n",
    "    {'response': y_binary_val_comb, 'reaction_time': y_time_val_comb},  \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "print(f'Training Loss: {train_results[0]}')\n",
    "print(f'Training Response prediction Acc: {train_results[2]}')\n",
    "print(f'Training RT MSE: {train_results[1]}')\n",
    "\n",
    "print(f'Val Loss: {val_results[0]}')\n",
    "print(f'Val Response prediction Acc: {val_results[2]}')\n",
    "print(f'Val RT MSE: {val_results[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0fd009-b6c2-438c-b502-4d90913a5ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_comb, predictions_comb = evaluate_model(\n",
    "    model_combined_full, \n",
    "    X_full_val_comb,  \n",
    "    X_recent_val_comb,  \n",
    "    y_binary_val_comb,  \n",
    "    y_time_val_comb  \n",
    ")\n",
    "\n",
    "for key, value in metrics_comb.items():\n",
    "    \n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0bb697-c5a3-455b-9d15-32913acc291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_comb, predictions_comb = evaluate_model(\n",
    "    model_combined_full, \n",
    "    X_full_val_rdw,  \n",
    "    X_recent_val_rdw,  \n",
    "    y_binary_val_rdw,  \n",
    "    y_time_val_rdw  \n",
    ")\n",
    "\n",
    "for key, value in metrics_comb.items():\n",
    "    \n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20154719-42ae-469f-8aed-8bca8a8e6fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_iid, predictions_iid = evaluate_model(\n",
    "    model_combined_full, \n",
    "    X_full_val_iid,  \n",
    "    X_recent_val_iid,  \n",
    "    y_binary_val_iid,  \n",
    "    y_time_val_iid  \n",
    ")\n",
    "\n",
    "print('metrics_iid validation')\n",
    "for key, value in metrics_iid.items():\n",
    "    \n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
