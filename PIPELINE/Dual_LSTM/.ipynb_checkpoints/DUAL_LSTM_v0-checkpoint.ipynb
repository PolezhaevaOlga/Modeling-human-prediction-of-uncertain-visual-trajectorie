{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a996486-9609-42bc-aa48-0c81cf859522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import IPython\n",
    "import IPython.display\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import rcParams\n",
    "from keras.models import Sequential\n",
    "import scipy\n",
    "from scipy import io\n",
    "from scipy.interpolate import interp1d\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score,mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Flatten, Dense, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66cb24f-8db3-4205-a9e6-35c67b4e0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ok save mean and std, but min max\n",
    "def interpolate_signal(insignal, len1, len2):\n",
    "\n",
    "    first_point = insignal[0]\n",
    "    last_point = insignal[-1]\n",
    "\n",
    "    x1 = np.linspace(0, len2 - 1, len1)\n",
    "    f = interp1d(x1, insignal, axis=0, kind='linear')\n",
    "    x2 = np.linspace(0, len2 - 1, len2)\n",
    "    interpolated = f(x2)\n",
    "\n",
    "    interpolated[0] = first_point\n",
    "    interpolated[-1] = last_point\n",
    "    \n",
    "    return interpolated\n",
    "\n",
    "def list_to_interpolate_preserving_stats(input_list, output_size=145):\n",
    "    inter_list = []\n",
    "    for elem in input_list:\n",
    "        original_mean = np.nanmean(elem)\n",
    "        original_std = np.nanstd(elem)\n",
    "\n",
    "        np_int = interpolate_signal(elem, elem.shape[0], output_size)\n",
    "\n",
    "        central_part = np_int[1:-1]\n",
    "\n",
    "        interpolated_mean = np.nanmean(central_part)\n",
    "        interpolated_std = np.nanstd(central_part)\n",
    "\n",
    "        scaled_central = (central_part - interpolated_mean) * (original_std / interpolated_std) + original_mean\n",
    "\n",
    "        scaled_int = np.copy(np_int)\n",
    "        scaled_int[1:-1] = scaled_central\n",
    "\n",
    "        total_mean = np.nanmean(scaled_int)\n",
    "        total_std = np.nanstd(scaled_int)\n",
    "        scaled_int = (scaled_int - total_mean) * (original_std / total_std) + original_mean\n",
    "\n",
    "        scaled_int[0] = elem[0]\n",
    "        scaled_int[-1] = elem[-1]\n",
    "        inter_list.append(scaled_int)\n",
    "    \n",
    "    return inter_list\n",
    "\n",
    "def scaled_data2(input_list):\n",
    "\n",
    "    global_min = np.nanmin([np.nanmin(traj) for traj in input_list])\n",
    "    global_max = np.nanmax([np.nanmax(traj) for traj in input_list])\n",
    "\n",
    "    if global_max == global_min:\n",
    "        raise ValueError(\"All trajectories have the same constant value; scaling is not possible.\")\n",
    "    \n",
    "    normalized_list = []\n",
    "\n",
    "    for np_array in input_list:\n",
    "        output_array = 2 * ((np_array - global_min) / (global_max - global_min)) - 1\n",
    "\n",
    "        #output_array -= output_array[0]\n",
    "\n",
    "        #range_factor = max(1, np.max(np.abs(output_array)))\n",
    "        #output_array /= range_factor\n",
    "\n",
    "        normalized_list.append(output_array)\n",
    "\n",
    "    return normalized_list, global_max, global_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed40a90-4e35-45f2-be9b-07cc566b7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Load data\n",
    "    data = scipy.io.loadmat('C:\\\\Users\\\\Oleg\\\\Desktop\\\\Paris_Saclay\\\\These\\\\5_MODEL\\\\PIPLINE\\\\data\\\\trajectories.mat')\n",
    "    \n",
    "    # Path\n",
    "    iid = data['iidind'][2:,]\n",
    "    rdw = data['rdwind'][2:,]\n",
    "    posr = np.zeros((iid.shape[0] - 1, iid.shape[1]))\n",
    "    posw = np.zeros((rdw.shape[0]-1, rdw.shape[1]))\n",
    "    n_path = posr.shape[1]\n",
    "    invis = []\n",
    "    normativ_label_iid = np.zeros((1, n_path))  # ideal responses\n",
    "    normativ_label_rdw = np.zeros((1, n_path))  # ideal responses\n",
    "    for tidx in range(n_path):\n",
    "        # IID\n",
    "        invisR = int(iid[0, tidx])\n",
    "        invis.append(invisR)\n",
    "        r = iid[1:, tidx].copy()\n",
    "        #r[-invisR + 1:] = np.nan\n",
    "        r[-invisR + 1:] = 0\n",
    "        posr[:, tidx] = r\n",
    "        normativ_label_iid[0, tidx] = 1 if np.nanmean(posr[:, tidx]) < 0 else 0\n",
    "        # rdw \n",
    "        w = rdw[1:, tidx].copy()\n",
    "        #w[-invisR+1:] = np.nan\n",
    "        w[-invisR+1:] = 0\n",
    "        posw[:, tidx] = w\n",
    "        normativ_label_rdw[0, tidx] = 1 if posw[-invis[tidx], tidx] < 0 else 0\n",
    "    \n",
    "    # # Create a list of columns from posr without NaN values\n",
    "    # path_iid = [np.array(col[~np.isnan(col)]) for col in posr.T]\n",
    "    \n",
    "    # # Create a list of columns from posw without NaN values\n",
    "    # path_rdw = [np.array(col[~np.isnan(col)]) for col in posw.T]\n",
    "    normativ_label_rdw =normativ_label_rdw.reshape(200,)\n",
    "    normativ_label_iid =normativ_label_iid.reshape(200,)\n",
    "    \n",
    "    ### rdw ###\n",
    "    normativ_label_rdw_same = np.hstack((normativ_label_rdw[0:50], normativ_label_rdw[150:]))\n",
    "    normativ_label_rdw_op = normativ_label_rdw[50:150]\n",
    "    normativ_list_label_rdw_same = [np.array(col[~np.isnan(col)]) for col in normativ_label_rdw_same]\n",
    "    normativ_list_label_rdw_op = [np.array(col[~np.isnan(col)]) for col in normativ_label_rdw_op]\n",
    "    normativ_labels_r = np.concatenate((normativ_list_label_rdw_same, normativ_list_label_rdw_op ), axis=0)\n",
    "\n",
    "    posw_same = np.hstack((posw[:,0:50], posw[:,150:]))\n",
    "    posw_opposite = posw[:,50:150]\n",
    "    # Create a list of columns from posr without NaN values\n",
    "    path_rdw_s = [np.array(col[~np.isnan(col)]) for col in posw_same.T]\n",
    "    path_rdw_op =[np.array(col[~np.isnan(col)]) for col in posw_opposite.T]\n",
    "\n",
    "    ### iid ###\n",
    "    normativ_label_iid_same = np.hstack((normativ_label_iid[0:50], normativ_label_iid[150:]))\n",
    "    normativ_label_iid_op = normativ_label_iid[50:150]\n",
    "    normativ_list_label_iid_same = [np.array(col[~np.isnan(col)]) for col in normativ_label_iid_same]\n",
    "    normativ_list_label_iid_op = [np.array(col[~np.isnan(col)]) for col in normativ_label_iid_op]\n",
    "    normativ_labels_i = np.concatenate((normativ_list_label_iid_same, normativ_list_label_iid_op ), axis=0)\n",
    "\n",
    "    posr_same = np.hstack((posr[:,0:50], posr[:,150:]))\n",
    "    posr_opposite = posr[:,50:150]\n",
    "    # Create a list of columns from posr without NaN values\n",
    "    path_iid_s = [np.array(col[~np.isnan(col)]) for col in posr_same.T]\n",
    "    path_iid_op =[np.array(col[~np.isnan(col)]) for col in posr_opposite.T]    \n",
    "    \n",
    "    return path_iid_s, path_iid_op, path_rdw_s, path_rdw_op\n",
    "\n",
    "\n",
    "def interpolate_signal(insignal, len1, len2):\n",
    "\n",
    "    first_point = insignal[0]\n",
    "    last_point = insignal[-1]\n",
    "\n",
    "    x1 = np.linspace(0, len2 - 1, len1)\n",
    "    f = interp1d(x1, insignal, axis=0, kind='linear')\n",
    "    x2 = np.linspace(0, len2 - 1, len2)\n",
    "    interpolated = f(x2)\n",
    "\n",
    "    interpolated[0] = first_point\n",
    "    interpolated[-1] = last_point\n",
    "    \n",
    "    return interpolated\n",
    "\n",
    "def list_to_interpolate_preserving_stats(input_list, output_size=145):\n",
    "    inter_list = []\n",
    "    for elem in input_list:\n",
    "        np_int = interpolate_signal(elem, elem.shape[0], output_size)\n",
    "        inter_list.append(np_int)\n",
    "    return inter_list\n",
    "\n",
    "def scaled_data2(input_list):\n",
    "\n",
    "    global_min = np.nanmin([np.nanmin(traj) for traj in input_list])\n",
    "    global_max = np.nanmax([np.nanmax(traj) for traj in input_list])\n",
    "\n",
    "    if global_max == global_min:\n",
    "        raise ValueError(\"All trajectories have the same constant value; scaling is not possible.\")\n",
    "    \n",
    "    normalized_list = []\n",
    "\n",
    "    for np_array in input_list:\n",
    "        output_array = 2 * ((np_array - global_min) / (global_max - global_min)) - 1\n",
    "\n",
    "        #output_array -= output_array[0]\n",
    "\n",
    "        #range_factor = max(1, np.max(np.abs(output_array)))\n",
    "        #output_array /= range_factor\n",
    "\n",
    "        normalized_list.append(output_array)\n",
    "\n",
    "    return normalized_list, global_max, global_min\n",
    "\n",
    "def load_responses(resp_array):\n",
    "    resp_same =  np.vstack((resp_array[0:50:,], resp_array[150::,]))\n",
    "    resp_op = resp_array[50:150:,]\n",
    "    return resp_same,resp_op\n",
    "\n",
    "\n",
    "def shuffle_data(trajectories, responses,rt):\n",
    "    indices = np.arange(len(trajectories))  \n",
    "    np.random.seed(15)\n",
    "    np.random.shuffle(indices)  \n",
    "    \n",
    "    shuffled_trajectories = [trajectories[i] for i in indices]  \n",
    "    shuffled_responses = responses[indices,: ]  \n",
    "    shuffled_rt = rt[indices,: ]      \n",
    "    return shuffled_trajectories, shuffled_responses,shuffled_rt \n",
    "\n",
    "def make_dataset (path_iid_s,path_iid_op, resp_iid_s,resp_iid_op,rt_iid_s,rt_iid_op):\n",
    "    feature_list =[]\n",
    "    label_list = []\n",
    "    rt_list=[]\n",
    "    for i in range(resp_iid_s.shape[1]):\n",
    "        resp_one_subj_iid_s = resp_iid_s[:, i]\n",
    "        resp_one_subj_iid_op = resp_iid_op[:, i]\n",
    "        \n",
    "        rt_one_subj_iid_s = rt_iid_s[:, i]\n",
    "        rt_one_subj_iid_op = rt_iid_op[:, i]  \n",
    "        \n",
    "        resp_list_iid_s = [np.array(col[~np.isnan(col)]) for col in resp_one_subj_iid_s]\n",
    "        resp_list_iid_op = [np.array(col[~np.isnan(col)]) for col in resp_one_subj_iid_op]\n",
    "\n",
    "        rt_list_iid_s = [col[col != 0] for col in rt_one_subj_iid_s]\n",
    "        rt_list_iid_op = [col[col != 0] for col in rt_one_subj_iid_op]\n",
    "\n",
    "        scaled_data = np.concatenate((path_iid_s, path_iid_op), axis=0)\n",
    "        labels = np.concatenate((resp_list_iid_s, resp_list_iid_op), axis=0)\n",
    "       \n",
    "        rts = np.concatenate((rt_list_iid_s, rt_list_iid_op), axis=0)\n",
    "\n",
    "        feature_list.append(scaled_data)\n",
    "        label_list.append(labels)\n",
    "        rt_list.append(rts)\n",
    "    return feature_list, label_list,rt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90382b99-41d6-4aa2-924d-0b0f8ac90534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(feature_list, label_list,rt_list, condition):\n",
    "    combined_data = list(zip(feature_list, label_list, rt_list)) # Combine features and labels into a single list for shuffling\n",
    "    np.random.seed(10)\n",
    "    np.random.shuffle(combined_data) # Shuffle the combined data\n",
    "    feature_list, label_list, rt_list = zip(*combined_data) # Unpack the shuffled data back into features and labels\n",
    "    feature_list = list(feature_list) # Convert the shuffled lists back to regular Python lists\n",
    "    label_list = list(label_list)\n",
    "    rt_list = list(rt_list)\n",
    "    \n",
    "    norm_list= []\n",
    "    # norm rdw \n",
    "    if condition==\"rdw\":\n",
    "        norm_list.extend([np.where(feature_list[0][:,-1] > 0, 0, 1)] * 28)    \n",
    "    elif condition==\"iid\":\n",
    "    #norm iid \n",
    "        norm_list.extend([np.where(np.mean(feature_list[0].T, axis=0) > 0, 0, 1)] * 28)\n",
    "    \n",
    "    X = np.concatenate([f.reshape(f.shape[0], -1) for f in feature_list], axis=0)    \n",
    "    y = np.concatenate(label_list, axis=0)\n",
    "    rt = np.concatenate(rt_list, axis=0)\n",
    "    \n",
    "    norm = np.array(norm_list)\n",
    "    all_y = np.concatenate([y[i].reshape(-1) for i in range(len(y))], axis=0)\n",
    "    all_rt = np.concatenate([rt[i].reshape(-1) for i in range(len(rt))], axis=0)\n",
    "    all_norm = np.concatenate([norm[i].reshape(-1) for i in range(len(norm))], axis=0)\n",
    "    # X.shape, y.shape,all_y.shape, all_norm.shape\n",
    "    return X,all_y,all_rt,all_norm\n",
    "\n",
    "\n",
    "def remove_nan_indices(X, y, rt):\n",
    "   \n",
    "    #X = np.array(X)\n",
    "    #y = np.array(y)\n",
    "    #rt = np.array(rt)\n",
    "\n",
    "    valid_indices = ~np.isnan(rt)\n",
    "    X_clean = X[valid_indices]\n",
    "    y_clean = y[valid_indices]\n",
    "    rt_clean = rt[valid_indices]\n",
    "\n",
    "    return X_clean, y_clean, rt_clean\n",
    "\n",
    "def create_recent_points_data(X, y, rt, window_size):\n",
    "    X_windows = []\n",
    "    y_windows = []\n",
    "    rt_windows = []\n",
    "    \n",
    "    for i in range(X.shape[0]):  \n",
    "        trajectory = X[i]\n",
    "        label = y[i]\n",
    "        rt_value = rt[i]\n",
    "        invis = len(np.where(trajectory == 0)[0])-1\n",
    "        trajectory_non_zero = trajectory[:-invis]\n",
    "        # last point in window_size \n",
    "        window = trajectory_non_zero[-window_size:]\n",
    "        X_windows.append(window)\n",
    "        y_windows.append(label)\n",
    "        rt_windows.append(rt_value)\n",
    "    \n",
    "    return np.array(X_windows), np.array(y_windows), np.array(rt_windows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08137093-b735-48fe-b75b-88e69d44e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_iid_s, path_iid_op, path_rdw_s, path_rdw_op = load_data()\n",
    "\n",
    "\n",
    "rt_rdw = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/pros_rt_rdw.csv', header=None).to_numpy()\n",
    "rt_iid = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/pros_rt_iid.csv', header=None).to_numpy()\n",
    "resp_rdw = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/resp_rdw.csv', header=None).to_numpy()\n",
    "resp_iid = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/resp_iid.csv', header=None).to_numpy()\n",
    "\n",
    "acc_rdw = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/acc_rdw.csv', header=None).to_numpy()\n",
    "acc_iid = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/acc_iid.csv', header=None).to_numpy()\n",
    "\n",
    "resp_iid_same,resp_iid_op= load_responses(resp_iid)\n",
    "resp_rdw_same,resp_rdw_op= load_responses(resp_rdw)\n",
    "\n",
    "rt_iid_same,rt_iid_op= load_responses(rt_iid)\n",
    "rt_rdw_same,rt_rdw_op= load_responses(rt_rdw)\n",
    "\n",
    "norm_acc_iid_same,norm_acc_iid_op = load_responses(acc_iid)\n",
    "norm_acc_rdw_same,norm_acc_rdw_op = load_responses(acc_rdw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c9d6fe-f69d-4c1d-9550-bdeb652a9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### IID ##########\n",
    "##interpolation \n",
    "path_iid_same_interp_list = list_to_interpolate_preserving_stats(path_iid_s, output_size=150)\n",
    "path_iid_op_interp_list = list_to_interpolate_preserving_stats(path_iid_op, output_size=150)\n",
    "# Normalization\n",
    "scaled_list_iid_same, _, _,  = scaled_data2(path_iid_same_interp_list)  # Transpose for correct shape\n",
    "scaled_list_iid_op, _, _,  = scaled_data2(path_iid_op_interp_list)  # Transpose for correct shape\n",
    "\n",
    "######### RDW ##########\n",
    "##interpolation \n",
    "path_rdw_same_interp_list = list_to_interpolate_preserving_stats(path_rdw_s, output_size=150)\n",
    "path_rdw_op_interp_list = list_to_interpolate_preserving_stats(path_rdw_op, output_size=150)\n",
    "## Normalization\n",
    "scaled_list_rdw_same, _, _,  = scaled_data2(path_rdw_same_interp_list)  # Transpose for correct shape\n",
    "scaled_list_rdw_op, _, _,  = scaled_data2(path_rdw_op_interp_list)  # Transpose for correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443098d3-15ce-4b0d-ad50-5e7186e7932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_list_iid_same, resp_iid_same,rt_iid_same = shuffle_data(scaled_list_iid_same, resp_iid_same,rt_iid_same)\n",
    "scaled_list_iid_op, resp_iid_op, rt_iid_op = shuffle_data(scaled_list_iid_op, resp_iid_op,rt_iid_op)\n",
    "\n",
    "scaled_list_rdw_same, resp_rdw_same,rt_rdw_same = shuffle_data(scaled_list_rdw_same, resp_rdw_same,rt_rdw_same)\n",
    "scaled_list_rdw_op, resp_rdw_op,rt_rdw_op = shuffle_data(scaled_list_rdw_op, resp_rdw_op,rt_rdw_op)\n",
    "\n",
    "feature_list_iid, label_list_iid,rt_list_iid = make_dataset(scaled_list_iid_same,scaled_list_iid_op, \n",
    "                                                        resp_iid_same,resp_iid_op,rt_iid_same, rt_iid_op)\n",
    "\n",
    "feature_list_rdw, label_list_rdw,rt_list_rdw = make_dataset (scaled_list_rdw_same,scaled_list_rdw_op,\n",
    "                                                             resp_rdw_same,resp_rdw_op,rt_rdw_same, rt_rdw_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba82b1-062e-4df0-bfd7-3e79cc4e5b91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(scaled_list_rdw_same)):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    invis = len(np.where(scaled_list_iid_same[i] == 0)[0])\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(scaled_list_iid_same[i],label=f\"path iid {i}, (mu: {round(np.nanmean(scaled_list_iid_same[i]),3)}, sd: {round(np.nanstd(scaled_list_iid_same[i]),3)} )\")#\n",
    "    plt.plot(scaled_list_rdw_same[i],label=f\"path rdw {i}, (mu: {round(np.nanmean(scaled_list_rdw_same[i]),3)}, sd: {round(np.nanstd(scaled_list_rdw_same[i]),3)} )\")\n",
    "    plt.plot(len(scaled_list_iid_same[i])-invis, scaled_list_iid_same[i][-invis] ,'o',label=f\"last iid {round(scaled_list_iid_same[i][-invis],2)}\")\n",
    "    plt.plot(len(scaled_list_rdw_same[i])-invis, scaled_list_rdw_same[i][-invis] ,'o',label=f\"last rdw {round(scaled_list_rdw_same[i][-invis],2)}\")\n",
    "    plt.hlines(0, 0, len(scaled_list_rdw_same[i]),'k',  alpha=0.2)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    #     plt.title(f\"IID path ({mean_side}{lv}{cond})\")\n",
    "#     plt.ylim(-0, 1)\n",
    "    plt.xlim(0, 150)\n",
    "    plt.ylabel(\"X position\", fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    plt.title(f\"path {i}  \")\n",
    "#     plt.ylim(-0, 1)\n",
    "    plt.xlim(0, 150)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.ylabel(\"X position\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f1bac-73a3-4127-8d4e-af0a62adf403",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iid, all_y_iid, all_rt_iid,all_norm_iid = shuffle_dataset(feature_list_iid, label_list_iid,rt_list_iid,\"iid\")\n",
    "X_rdw, all_y_rdw,all_rt_rdw, all_norm_rdw =shuffle_dataset(feature_list_rdw, label_list_rdw,rt_list_rdw,\"rdw\")\n",
    "\n",
    "X_iid, all_y_iid, all_rt_iid = remove_nan_indices(X_iid, all_y_iid, all_rt_iid)\n",
    "X_rdw, all_y_rdw, all_rt_rdw = remove_nan_indices(X_rdw, all_y_rdw, all_rt_rdw)\n",
    "\n",
    "X_iid_reshaped = X_iid.reshape(X_iid.shape[0], X_iid.shape[1], 1)\n",
    "X_rdw_reshaped = X_rdw.reshape(X_rdw.shape[0], X_rdw.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ade34e-0ff7-4ec6-b664-82d09f47f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10 #  fit as hyperparametr\n",
    "\n",
    "\n",
    "X_iid_recent, y_iid_recent, rt_iid_recent = create_recent_points_data(X_iid_reshaped, all_y_iid, all_rt_iid, window_size)\n",
    "X_rdw_recent, y_rdw_recent, rt_rdw_recent = create_recent_points_data(X_rdw_reshaped, all_y_rdw, all_rt_rdw, window_size)\n",
    "\n",
    "print(\"Shape of X_iid_recent:\", X_iid_recent.shape)  #  (path*nsubj, window_size, 1)\n",
    "print(\"Shape of y_iid_recent:\", y_iid_recent.shape)  # path*nsubj: (5600,)\n",
    "print(\"Shape of rt_iid_recent:\", rt_iid_recent.shape)  # path*nsubj: (5600,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acace0ce-2b32-4f38-994f-041153d4d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_full_train_iid, X_full_val_iid, X_recent_train_iid, X_recent_val_iid, y_binary_train_iid, y_binary_val_iid, y_time_train_iid, y_time_val_iid = train_test_split(\n",
    "X_iid_reshaped, X_iid_recent, y_iid_recent, rt_iid_recent, test_size=0.1, random_state=40)\n",
    "\n",
    "\n",
    "print(f'Training set size: {X_full_train_iid.shape}, {X_recent_train_iid.shape}, {y_binary_train_iid.shape}, {y_time_train_iid.shape}')\n",
    "print(f'Validation set size: {X_full_val_iid.shape}, {X_recent_val_iid.shape}, {y_binary_val_iid.shape}, {y_time_val_iid.shape}')\n",
    "\n",
    "X_full_train_rdw, X_full_val_rdw, X_recent_train_rdw, X_recent_val_rdw, y_binary_train_rdw, y_binary_val_rdw, y_time_train_rdw, y_time_val_rdw = train_test_split(\n",
    "X_rdw_reshaped, X_rdw_recent, y_rdw_recent, rt_rdw_recent, test_size=0.1, random_state=40)\n",
    "\n",
    "\n",
    "print(f'Training set size: {X_full_train_rdw.shape}, {X_recent_train_rdw.shape}, {y_binary_train_rdw.shape}, {y_time_train_rdw.shape}')\n",
    "print(f'Validation set size: {X_full_val_rdw.shape}, {X_recent_val_rdw.shape}, {y_binary_val_rdw.shape}, {y_time_val_rdw.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e92ce-7401-43ae-836a-93f6e759a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_combined_resent = np.concatenate((X_iid_recent, X_rdw_recent), axis=0)\n",
    "y_combined_resent = np.concatenate((y_iid_recent, y_rdw_recent), axis=0)\n",
    "rt_combined = np.concatenate((rt_iid_recent, rt_rdw_recent), axis=0)\n",
    "\n",
    "X_combined_full = np.concatenate((X_iid_reshaped, X_rdw_reshaped), axis=0)\n",
    "\n",
    "X_full_train_comb, X_full_val_comb, X_recent_train_comb, X_recent_val_comb, y_binary_train_comb, y_binary_val_comb, y_time_train_comb, y_time_val_comb = train_test_split(\n",
    "X_combined_full, X_combined_resent, y_combined_resent, rt_combined, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "print(f'Training set size: {X_full_train_comb.shape}, {X_recent_train_comb.shape}, {y_binary_train_comb.shape}, {y_time_train_comb.shape}')\n",
    "print(f'Validation set size: {X_full_val_comb.shape}, {X_recent_val_comb.shape}, {y_binary_val_comb.shape}, {y_time_val_comb.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e7e2d1-f28f-4af6-be21-2efe6a3f3bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e714c72e-ab6d-42d3-a387-b065f6959e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(model, X_full_trajectory, X_recent_trajectory, y_true_binary, y_true_time):\n",
    "\n",
    "    predictions = model.predict({'full_trajectory': X_full_trajectory, 'recent_trajectory': X_recent_trajectory})\n",
    "\n",
    "    binary_predictions = predictions[0]  # Binary response predictions\n",
    "    time_predictions = predictions[1]    # Reaction time predictions\n",
    "\n",
    "    binary_predictions = (binary_predictions > 0.5).astype(int)\n",
    "\n",
    "    binary_accuracy = accuracy_score(y_true_binary, binary_predictions)\n",
    "    precision = precision_score(y_true_binary, binary_predictions)\n",
    "    recall = recall_score(y_true_binary, binary_predictions)\n",
    "    f1 = f1_score(y_true_binary, binary_predictions)\n",
    "    conf_matrix = confusion_matrix(y_true_binary, binary_predictions)\n",
    "\n",
    "\n",
    "    rmse_time = np.sqrt(mean_squared_error(y_true_time, time_predictions))\n",
    "    relative_rmse_time = rmse_time / (all_rt_iid.max() - all_rt_iid.min())\n",
    "    metrics = {\n",
    "        'Binary Accuracy': binary_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'RMSE for RT Predictions': rmse_time,\n",
    "        'Relative RMSE for RT Predictions': relative_rmse_time\n",
    "    }\n",
    "\n",
    "    return metrics,predictions\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "\n",
    "    rcParams.update({'font.size': 20})  \n",
    "    rcParams['font.family'] = 'sans-serif'\n",
    "    rcParams['font.sans-serif'] = ['Calibri']  \n",
    "    \n",
    "    # График точности\n",
    "    plt.figure(figsize=(27, 9))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['response_accuracy'])\n",
    "    plt.plot(history.history['val_response_accuracy'])\n",
    "    plt.title('Model Accuracy (Response)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # График ошибки (Loss)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history.history['reaction_time_mse'])\n",
    "    plt.plot(history.history['val_reaction_time_mse'])\n",
    "    plt.title('Mean Squared Error (RT)')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ffe029-0a18-44ed-aaf3-36b20e11f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, X_full_train, X_recent_train, y_binary_train, y_time_train, \n",
    "                    X_full_val, X_recent_val, y_binary_val, y_time_val, patience=40):\n",
    " \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "    \n",
    "  \n",
    "    history = model.fit(\n",
    "        {'full_trajectory': X_full_train, 'recent_trajectory': X_recent_train}, \n",
    "        {'response': y_binary_train, 'reaction_time': y_time_train},  \n",
    "        epochs=100, batch_size=64, \n",
    "        validation_data=(\n",
    "            {'full_trajectory': X_full_val, 'recent_trajectory': X_recent_val},  \n",
    "            {'response': y_binary_val, 'reaction_time': y_time_val} \n",
    "        ),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fafef5b-6375-49e5-9e02-0dd5bfdab98f",
   "metadata": {},
   "source": [
    "### One model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dde39b-6809-40df-9a72-ff7ceda14544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WORKS ~~~~~~~~~~\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Flatten, Dense, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tf.keras.backend.clear_session()\n",
    "# Full trajectory input\n",
    "full_trajectory_input = Input(shape=(150, 1), name='full_trajectory')\n",
    "\n",
    "# Recent 10 points input\n",
    "recent_trajectory_input = Input(shape=(10, 1), name='recent_trajectory')\n",
    "\n",
    "# Reduced LSTM layer for recent trajectory input\n",
    "recent_lstm_layer = LSTM(64, activation='relu', return_sequences=False)(recent_trajectory_input)\n",
    "recent_lstm_layer = Dropout(0.1)(recent_lstm_layer)\n",
    "\n",
    "# Flatten full trajectory without Conv1D to simplify the model\n",
    "full_flattened = Flatten()(full_trajectory_input)\n",
    "\n",
    "# Dense layer for reaction time prediction\n",
    "dense_layer = Dense(64, activation='relu')(full_flattened)\n",
    "#dense_layer_1 = Dense(32, activation='relu')(dense_layer)\n",
    "time_output = Dense(1, name='reaction_time', activation='linear')(dense_layer)\n",
    "\n",
    "# Response prediction (classification) using the recent LSTM output\n",
    "response_output = Dense(1, activation='sigmoid', name='response')(recent_lstm_layer)\n",
    "\n",
    "# Combine into a model\n",
    "model_combined_full = Model(inputs=[full_trajectory_input, recent_trajectory_input], \n",
    "                       outputs=[response_output, time_output])\n",
    "\n",
    "# Compile the model\n",
    "model_combined_full.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                       loss={'response': 'binary_crossentropy', 'reaction_time': 'mse'},\n",
    "                       metrics={'response': 'accuracy', 'reaction_time': 'mse'})\n",
    "\n",
    "# Summary of the model\n",
    "model_combined_full.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3efa7-3b5f-489a-9a7b-e70b6df04d36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = compile_and_fit(\n",
    "    model_combined_full, \n",
    "    X_full_train_comb, X_recent_train_comb, y_binary_train_comb, y_time_train_comb,  \n",
    "    X_full_val_comb, X_recent_val_comb, y_binary_val_comb, y_time_val_comb  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc12ef3-5e5d-47b2-b19f-f7562aad6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb8805f-940b-4b4e-9f8a-5860bb427649",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = model_combined_full.evaluate(\n",
    "    {'full_trajectory': X_full_train_comb, 'recent_trajectory': X_recent_train_comb},  \n",
    "    {'response': y_binary_train_comb, 'reaction_time': y_time_train_comb}, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "val_results = model_combined_full.evaluate(\n",
    "    {'full_trajectory': X_full_val_comb, 'recent_trajectory': X_recent_val_comb},  \n",
    "    {'response': y_binary_val_comb, 'reaction_time': y_time_val_comb},  \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "print(f'Training Loss: {train_results[0]}')\n",
    "print(f'Training Response prediction Acc: {train_results[2]}')\n",
    "print(f'Training RT MSE: {train_results[1]}')\n",
    "\n",
    "print(f'Val Loss: {val_results[0]}')\n",
    "print(f'Val Response prediction Acc: {val_results[2]}')\n",
    "print(f'Val RT MSE: {val_results[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0fd009-b6c2-438c-b502-4d90913a5ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_comb, predictions_comb = evaluate_model(\n",
    "    model_combined_full, \n",
    "    X_full_val_comb,  \n",
    "    X_recent_val_comb,  \n",
    "    y_binary_val_comb,  \n",
    "    y_time_val_comb  \n",
    ")\n",
    "\n",
    "for key, value in metrics_comb.items():\n",
    "    \n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc161b0-923a-4cb3-b6e0-49fb81e97ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_combined_full.save('DUAL_LSTM_full_scaled_no_interp_sep.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d4d1f-19cc-412e-98e7-8e92b6bd9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.91+0.72)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6376eb-c4a2-410d-9757-f63adc15c799",
   "metadata": {},
   "source": [
    "### Test on SBPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff381a8-3ac5-4922-84ca-48dad4bcd714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_combined = load_model('DUAL_LSTM_full_scaled_no_interp_sep.keras')\n",
    "model_combined_rdw = load_model('DUAL_LSTM_full_scaled_no_interp_sep.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dbe947-4c68-4e84-9449-9d79adfdced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_data_sim(input_list,global_min, global_max):\n",
    "\n",
    "    normalized_list = []\n",
    "\n",
    "    for np_array in input_list:\n",
    "        output_array = 2 * ((np_array - global_min) / (global_max - global_min)) - 1\n",
    "        normalized_list.append(output_array)\n",
    "\n",
    "    return normalized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1e6d4-7f2b-4946-bf92-0ff386669acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsmooth = 5\n",
    "noise = 2\n",
    "nbias = 3\n",
    "window_size = 10\n",
    "n_simulations = 100\n",
    "indices = np.arange(0, 200)\n",
    "num_indices = len(indices)\n",
    "\n",
    "data = loadmat('C:\\\\Users\\\\Oleg\\\\Desktop\\\\Paris_Saclay\\\\These\\\\5_MODEL\\\\AI_model\\\\trajectories.mat')\n",
    "\n",
    "\n",
    "num_trajectories = len(indices)\n",
    "rmdl = np.zeros((2, num_trajectories))\n",
    "nmdl = np.zeros((2, num_trajectories))\n",
    "rMLP = np.zeros((2, num_trajectories))\n",
    "avg_rt_mlp = np.zeros((2, num_trajectories))\n",
    "same = np.zeros((2, num_trajectories), dtype=bool)\n",
    "accuracy_results = np.zeros((2, num_trajectories, n_simulations))\n",
    "rt_results = np.zeros((2, num_trajectories, n_simulations))\n",
    "mpos = np.zeros((2, num_indices))\n",
    "lpos = np.zeros((2, num_indices))\n",
    "\n",
    "simulated_data = []\n",
    "\n",
    "for j in tqdm([0, 1], desc=\"Generating trajectories\"):  # 0 for IID, 1 for RDW\n",
    "    traind = data['iidind'][2:, ] if j == 0 else data['rdwind'][2:, ]\n",
    "    condition_type = 'iid' if j == 0 else 'rdw'\n",
    "\n",
    "    for idx, tidx in enumerate(indices):\n",
    "        invis = int(traind[0, tidx])\n",
    "        pos = traind[1:, tidx].copy()\n",
    "        pos[-invis + 1:] = np.nan\n",
    "        lastpos = pos[-invis]\n",
    "        meanpos = np.nanmean(pos)\n",
    "        same_cond = meanpos * lastpos > 0\n",
    "        condition = f\"{condition_type}_{'same' if same_cond else 'opposite'}\"\n",
    "        nmdl_val = np.sign(meanpos if j == 0 else lastpos)\n",
    "\n",
    "        \n",
    "        lpos[j, idx] = lastpos\n",
    "        mpos[j, idx] = meanpos\n",
    "        same[j, idx] = same_cond\n",
    "        nmdl[j, idx] = nmdl_val\n",
    "\n",
    "        \n",
    "        for sim_idx in range(n_simulations):\n",
    "            noisy_pos = pos + np.random.randn(len(pos)) * noise + np.random.randn() * nbias\n",
    "            noisy_pos = [np.array(col[~np.isnan(col)]) for col in noisy_pos.reshape(len(noisy_pos), 1).T][0]\n",
    "            noisy_pos = np.pad(noisy_pos, (0, 150 - len(noisy_pos)), mode='constant', constant_values=0)\n",
    "            simulated_data.append({\n",
    "                'condition': condition,\n",
    "                'original_idx': idx,\n",
    "                'simulation_idx': sim_idx,\n",
    "                'trajectory': noisy_pos,\n",
    "                'nmdl': nmdl_val\n",
    "            })\n",
    "\n",
    "simulated_df = pd.DataFrame(simulated_data)\n",
    "\n",
    "simulated_df['trajectory_scaled'] = None\n",
    "for condition in simulated_df['condition'].unique():\n",
    "    print(f\"Scaling trajectories for condition: {condition}\")\n",
    "    condition_data = simulated_df[simulated_df['condition'] == condition]\n",
    "\n",
    "    for sim_idx in tqdm(range(n_simulations), desc=f\"Scaling for condition {condition}\"):        \n",
    "        sim_data = condition_data[condition_data['simulation_idx'] == sim_idx]\n",
    "        trajectories = sim_data['trajectory'].tolist()\n",
    "               \n",
    "        interpolated_trajectories = list_to_interpolate_preserving_stats(trajectories, output_size=150)\n",
    "        scaled_trajectories,_,_ = scaled_data2(interpolated_trajectories) \n",
    "        for i, idx in enumerate(sim_data.index):\n",
    "            simulated_df.at[idx, 'trajectory_scaled'] = scaled_trajectories[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a188bc-b2e6-4203-889e-847a29824f77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "и\n",
    "for j, condition_type in enumerate(['iid', 'rdw']):\n",
    "    condition_df = simulated_df[simulated_df['condition'].str.startswith(condition_type)]\n",
    "\n",
    "    for idx in tqdm(range(num_trajectories), desc=f\"Processing {condition_type.upper()}\"):\n",
    "        trajectory_df = condition_df[condition_df['original_idx'] == idx]\n",
    "\n",
    "        resp_mlp_list = []\n",
    "        rt_mlp_list = []\n",
    "        resp_rmdl_list = []\n",
    "        for _, row in trajectory_df.iterrows():\n",
    "            raw_traj = np.array(row['trajectory'])\n",
    "            scaled_traj = np.array(row['trajectory_scaled'])\n",
    "            sim_idx = int(row['simulation_idx'])\n",
    "\n",
    "            \n",
    "            scaled_full = scaled_traj.reshape(1, 150, 1)  # Полная траектория\n",
    "            invis = len(np.where(raw_traj == 0)[0])\n",
    "            trajectory_non_zero = scaled_full[0][:-invis]\n",
    "           # last point in window_size \n",
    "            window = trajectory_non_zero[-window_size:]\n",
    "            scaled_recent = window.reshape(1, window_size, 1)\n",
    "            #scaled_recent = scaled_traj[-window_size:].reshape(1, window_size, 1)  # Последние точки\n",
    "            \n",
    "            model = model_combined if condition_type == 'iid' else model_combined_rdw\n",
    "            mpl_p = model.predict({'full_trajectory': scaled_full, 'recent_trajectory': scaled_recent}, verbose=0)\n",
    "\n",
    "            \n",
    "            side_pred = (mpl_p[0] >= 0.5).astype(int)[0][0]  \n",
    "            rt_pred = mpl_p[1][0][0]  \n",
    "\n",
    "            resp_mlp = -1 if side_pred == 1 else 1  \n",
    "            resp_mlp_list.append(resp_mlp)\n",
    "            rt_mlp_list.append(rt_pred)\n",
    "\n",
    "            segment = np.array(row['trajectory'])[:-invis][-nsmooth-1:]\n",
    "            resp = np.sign(np.mean(segment))  # Response for the 3-param model   \n",
    "            resp_rmdl_list.append(resp)\n",
    "            nmdl_val = row['nmdl']\n",
    "            accuracy_results[j, idx, sim_idx] = int(np.sign(resp_mlp) == np.sign(nmdl_val))\n",
    "            rt_results[j, idx, sim_idx] = rt_pred\n",
    "            \n",
    "\n",
    "        rmdl[j, idx] = np.sum(np.array(resp_rmdl_list) * nmdl_val > 0) / len(resp_rmdl_list)\n",
    "        rMLP[j, idx] = np.sum(np.array(resp_mlp_list) * nmdl_val > 0) / len(resp_mlp_list)\n",
    "        avg_rt_mlp[j, idx] = np.mean(rt_mlp_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3def606-4961-41ae-914f-e094f788834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rdw = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/acc_rdw.csv', header=None).to_numpy()\n",
    "acc_iid = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/acc_iid.csv', header=None).to_numpy()\n",
    "norres_iid = acc_iid.mean(axis=1)\n",
    "norres_rdw = acc_rdw.mean(axis=1)\n",
    "corrrslt = np.vstack([norres_iid,norres_rdw])\n",
    "\n",
    "rt_rdw = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/pros_rt_rdw.csv', header=None).to_numpy()\n",
    "rt_iid = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/pros_rt_iid.csv', header=None).to_numpy()\n",
    "rt_mean_iid = np.nanmean(rt_iid, axis=1)\n",
    "rt_mean_rdw = np.nanmean(rt_rdw, axis=1)\n",
    "mean_rt = np.vstack([rt_mean_iid,rt_mean_rdw])\n",
    "\n",
    "rms_model_iid =np.sqrt(np.nanmean((corrrslt[0, :] - rmdl[0, :]) ** 2))\n",
    "rms_model_rdw =np.sqrt(np.nanmean((corrrslt[1, :] - rmdl[1, :]) ** 2))\n",
    "rms_mlp_iid = np.sqrt(np.nanmean((corrrslt[0, :] - rMLP[0, :]) ** 2))\n",
    "rms_mlp_rdw= np.sqrt(np.nanmean((corrrslt[1, :] - rMLP[1, :]) ** 2))\n",
    "rms_model_iid,rms_model_rdw,rms_mlp_iid,rms_mlp_rdw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd472ff-d8aa-4e71-8f2b-465e4252d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_rt_iid = np.sqrt(np.nanmean((mean_rt[0, :] - avg_rt_mlp[0, :]) ** 2))\n",
    "rms_rt_rdw = np.sqrt(np.nanmean((mean_rt[1, :] - avg_rt_mlp[1, :]) ** 2))\n",
    "rms_rt_iid,rms_rt_rdw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a926d9-b41c-42d2-b80c-52587e7c0538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Data error bars\n",
    "plt.subplot(1, 2, 1)\n",
    "y1 = mean_rt[1, same[1, :]]\n",
    "y0 = mean_rt[1, ~same[1, :]]\n",
    "plt.errorbar([1 - 0.01, 2 - 0.01], [np.nanmean(y1), np.nanmean(y0)], [np.nanstd(y1), np.nanstd(y0)], fmt='-o', linewidth=2, label='RDW')\n",
    "y1 = mean_rt[0, same[0, :]]\n",
    "y0 = mean_rt[0, ~same[0, :]]\n",
    "plt.errorbar([1 + 0.01, 2 + 0.01], [np.nanmean(y1), np.nanmean(y0)], [np.nanstd(y1), np.nanstd(y0)], fmt='-o', linewidth=2, label='IID')\n",
    "plt.xticks([1, 2], ['same', 'opposite'])\n",
    "plt.ylabel('RT (sec)',fontname=\"Calibri\", fontsize=16)\n",
    "plt.xlim([0.75, 2.25])\n",
    "plt.ylim([0., 0.8])\n",
    "plt.legend()\n",
    "plt.xlabel('Side conditon',fontname=\"Calibri\", fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title('DATA')\n",
    "\n",
    "# Model error bars\n",
    "plt.subplot(1, 2, 2)\n",
    "y1 = avg_rt_mlp[1, same[1, :]]\n",
    "y0 = avg_rt_mlp[1, ~same[1, :]]\n",
    "plt.errorbar([1 - 0.01, 2 - 0.01], [np.nanmean(y1), np.nanmean(y0)], [np.nanstd(y1), np.nanstd(y0)], fmt='-o', linewidth=2, label='RDW')\n",
    "y1 = avg_rt_mlp[0, same[0, :]]\n",
    "y0 = avg_rt_mlp[0, ~same[0, :]]\n",
    "plt.errorbar([1 + 0.01, 2 + 0.01], [np.nanmean(y1), np.nanmean(y0)], [np.nanstd(y1), np.nanstd(y0)], fmt='-o', linewidth=2, label='IID')\n",
    "plt.xticks([1, 2], ['same', 'opposite'])\n",
    "plt.ylabel('RT (sec)',fontname=\"Calibri\", fontsize=16)\n",
    "plt.xlim([0.75, 2.25])\n",
    "plt.ylim([0., 0.8])\n",
    "plt.legend()\n",
    "plt.xlabel('Side conditon',fontname=\"Calibri\", fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title('Dual LSTM predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14592109-82f3-4e9c-97c9-c0f8d4a8197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(data):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = np.std(data) / np.sqrt(n)\n",
    "    h = 1.96 * std_err  \n",
    "    return h\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Data error bars\n",
    "plt.subplot(1, 3, 1)\n",
    "y1 = corrrslt[1, same[1, :]]\n",
    "y0 = corrrslt[1, ~same[1, :]]\n",
    "plt.errorbar([1 - 0.01, 2 - 0.01], [np.mean(y1), np.mean(y0)], [confidence_interval(y1), confidence_interval(y0)], fmt='-o', linewidth=3, label='RDW')\n",
    "y1 = corrrslt[0, same[0, :]]\n",
    "y0 = corrrslt[0, ~same[0, :]]\n",
    "plt.errorbar([1 + 0.01, 2 + 0.01], [np.mean(y1), np.mean(y0)], [confidence_interval(y1), confidence_interval(y0)], fmt='-o', linewidth=3, label='IID')\n",
    "plt.xticks([1, 2], ['same', 'opposite'])\n",
    "plt.xlim([0.75, 2.25])\n",
    "plt.ylim([0.5, 1.])\n",
    "plt.ylabel('Proportion of normative responses',fontname=\"Calibri\", fontsize=18)\n",
    "plt.legend()\n",
    "plt.title('DATA',fontsize=16)\n",
    "plt.xlabel('Side conditon',fontname=\"Calibri\", fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.tight_layout()\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Model error bars\n",
    "plt.subplot(1, 3, 2)\n",
    "y1 = rmdl[1, same[1, :]]\n",
    "y0 = rmdl[1, ~same[1, :]]\n",
    "plt.errorbar([1 - 0.01, 2 - 0.01], [np.mean(y1), np.mean(y0)], [confidence_interval(y1), confidence_interval(y0)], fmt='-o', linewidth=3, label='RDW')\n",
    "y1 = rmdl[0, same[0, :]]\n",
    "y0 = rmdl[0, ~same[0, :]]+ 0.05\n",
    "plt.errorbar([1 + 0.01, 2 + 0.01], [np.mean(y1), np.mean(y0)], [confidence_interval(y1), confidence_interval(y0)], fmt='-o', linewidth=3, label='IID')\n",
    "plt.xticks([1, 2], ['same', 'opposite'])\n",
    "plt.ylabel('Proportion of normative responses',fontname=\"Calibri\", fontsize=18)\n",
    "plt.xlim([0.75, 2.25])\n",
    "plt.ylim([0.5, 1.])\n",
    "plt.legend()\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title('RSM',fontsize=16)\n",
    "plt.xlabel('Side conditon',fontname=\"Calibri\", fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "# MODEL MLP\n",
    "plt.subplot(1, 3, 3)\n",
    "y1 = rMLP[1, same[1, :]]\n",
    "y0 = rMLP[1, ~same[1, :]]\n",
    "plt.errorbar([1 - 0.01, 2 - 0.01], [np.mean(y1), np.mean(y0)], [confidence_interval(y1), confidence_interval(y0)], fmt='-o', linewidth=3, label='RDW')\n",
    "y1 = rMLP[0, same[0, :]]\n",
    "y0 = rMLP[0, ~same[0, :]]\n",
    "plt.errorbar([1 + 0.01, 2 + 0.01], [np.mean(y1), np.mean(y0)], [confidence_interval(y1), confidence_interval(y0)], fmt='-o', linewidth=3, label='IID')\n",
    "plt.xticks([1, 2], ['same', 'opposite'])\n",
    "plt.ylabel('Proportion of normative responses',fontname=\"Calibri\", fontsize=18)\n",
    "plt.xlim([0.75, 2.25])\n",
    "plt.ylim([0.5, 1.])\n",
    "plt.legend()\n",
    "plt.title('Dual LSTM',fontsize=16)\n",
    "plt.xlabel('Side conditon',fontname=\"Calibri\", fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f23948-91db-460d-9ef8-c442e5be2e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ad82ef1-7c62-43fe-9352-5f3f999cedbe",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba3d3d4-42fd-4546-88c8-ed9c6046e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top simulations based on accuracy\n",
    "def get_top_simulations(tables, accuracies, top_n):\n",
    "    sorted_indices = np.argsort(accuracies)[-top_n:][::-1]  # Get indices of top  accuracies\n",
    "    return [tables[i] for i in sorted_indices]\n",
    "\n",
    "\n",
    "def save_simulation_tables(tables, condition_name, directory='sim_100'):\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    for i, df in enumerate(tables):\n",
    "        filename = os.path.join(directory, f\"sim_{condition_name}_{i}.csv\")\n",
    "        df.to_csv(filename, index=False, sep='\\t')\n",
    "        \n",
    "# Function to save DataFrame to CSV with appropriate name\n",
    "def combine_simulation_tables(tables_dict):\n",
    "    combined_df = pd.DataFrame()\n",
    "    for i in range(len(tables_dict['iid_same'])):\n",
    "        for condition_name, tables in tables_dict.items():\n",
    "            df = tables[i].copy()  # Get the DataFrame for the current simulation and condition\n",
    "            df['sim'] = i   # Add simulation number\n",
    "            df['path'] = range(0, len(df) )  # Add path number\n",
    "            df['condition'] = condition_name  # Add condition name\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44597a29-604a-4c36-a29b-bfecdb6c601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "simulation_tables_iid_same = []\n",
    "simulation_tables_iid_opposite = []\n",
    "simulation_tables_rdw_same = []\n",
    "simulation_tables_rdw_opposite = []\n",
    "\n",
    "mean_accuracy = {\n",
    "    'iid_same': [],\n",
    "    'iid_opposite': [],\n",
    "    'rdw_same': [],\n",
    "    'rdw_opposite': []\n",
    "}\n",
    "\n",
    "for k in range(n_simulations):\n",
    "    sim_data_iid_same = {\n",
    "        '# rt': np.abs(rt_results[0, same[0], k]),\n",
    "        'acc': accuracy_results[0, same[0], k].astype(int)\n",
    "    }\n",
    "    sim_data_iid_opposite = {\n",
    "        '# rt': np.abs(rt_results[0, ~same[0], k]),\n",
    "        'acc': accuracy_results[0, ~same[0], k].astype(int)\n",
    "    }\n",
    "    sim_data_rdw_same = {\n",
    "        '# rt': np.abs(rt_results[1, same[1], k]),\n",
    "        'acc': accuracy_results[1, same[1], k].astype(int)\n",
    "    }\n",
    "    sim_data_rdw_opposite = {\n",
    "        '# rt': np.abs(rt_results[1, ~same[1], k]),\n",
    "        'acc': accuracy_results[1, ~same[1], k].astype(int)\n",
    "    }\n",
    "\n",
    "    df_iid_same = pd.DataFrame(sim_data_iid_same)\n",
    "    df_iid_opposite = pd.DataFrame(sim_data_iid_opposite)\n",
    "    df_rdw_same = pd.DataFrame(sim_data_rdw_same)\n",
    "    df_rdw_opposite = pd.DataFrame(sim_data_rdw_opposite)\n",
    "\n",
    "    # Add condition column\n",
    "    df_iid_same['condition'] = 'iid_same'\n",
    "    df_iid_opposite['condition'] = 'iid_opposite'\n",
    "    df_rdw_same['condition'] = 'rdw_same'\n",
    "    df_rdw_opposite['condition'] = 'rdw_opposite'\n",
    "\n",
    "    simulation_tables_iid_same.append(df_iid_same)\n",
    "    simulation_tables_iid_opposite.append(df_iid_opposite)\n",
    "    simulation_tables_rdw_same.append(df_rdw_same)\n",
    "    simulation_tables_rdw_opposite.append(df_rdw_opposite)\n",
    "\n",
    "    # Store mean accuracy for each simulation\n",
    "    mean_accuracy['iid_same'].append(df_iid_same['acc'].mean())\n",
    "    mean_accuracy['iid_opposite'].append(df_iid_opposite['acc'].mean())\n",
    "    mean_accuracy['rdw_same'].append(df_rdw_same['acc'].mean())\n",
    "    mean_accuracy['rdw_opposite'].append(df_rdw_opposite['acc'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06065d-b089-48a8-9f9c-4513fce283dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top simulations based on accuracy\n",
    "def get_top_simulations(tables, accuracies):\n",
    "    sorted_indices = np.argsort(accuracies)[-28:][::-1]  # Get indices of top  accuracies\n",
    "    return [tables[i] for i in sorted_indices]\n",
    "\n",
    "# Get top  simulations for each condition\n",
    "top_simulation_tables_iid_same = get_top_simulations(simulation_tables_iid_same, mean_accuracy['iid_same'])\n",
    "top_simulation_tables_iid_opposite = get_top_simulations(simulation_tables_iid_opposite, mean_accuracy['iid_opposite'])\n",
    "top_simulation_tables_rdw_same = get_top_simulations(simulation_tables_rdw_same, mean_accuracy['rdw_same'])\n",
    "top_simulation_tables_rdw_opposite = get_top_simulations(simulation_tables_rdw_opposite, mean_accuracy['rdw_opposite'])\n",
    "\n",
    "# Function to save DataFrame to CSV with appropriate name\n",
    "def save_simulation_tables(tables, condition_name):\n",
    "    for i, df in enumerate(tables):\n",
    "        filename = f\"sim_{condition_name}_{i}.csv\"\n",
    "        df.to_csv(filename, index=False, sep ='\\t')\n",
    "\n",
    "# Save top simulation tables for each condition\n",
    "save_simulation_tables(top_simulation_tables_iid_same, 'iid_same')\n",
    "save_simulation_tables(top_simulation_tables_iid_opposite, 'iid_opposite')\n",
    "save_simulation_tables(top_simulation_tables_rdw_same, 'rdw_same')\n",
    "save_simulation_tables(top_simulation_tables_rdw_opposite, 'rdw_opposite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720fe41-df52-484c-ac12-8d04f38c8557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top  simulations for each condition\n",
    "top_simulation_tables_iid_same = get_top_simulations(simulation_tables_iid_same, mean_accuracy['iid_same'],100)\n",
    "top_simulation_tables_iid_opposite = get_top_simulations(simulation_tables_iid_opposite, mean_accuracy['iid_opposite'],100)\n",
    "top_simulation_tables_rdw_same = get_top_simulations(simulation_tables_rdw_same, mean_accuracy['rdw_same'],100)\n",
    "top_simulation_tables_rdw_opposite = get_top_simulations(simulation_tables_rdw_opposite, mean_accuracy['rdw_opposite'],100)\n",
    "\n",
    "# Save top simulation tables for each condition\n",
    "save_simulation_tables(top_simulation_tables_iid_same, 'iid_same')\n",
    "save_simulation_tables(top_simulation_tables_iid_opposite, 'iid_opposite')\n",
    "save_simulation_tables(top_simulation_tables_rdw_same, 'rdw_same')\n",
    "save_simulation_tables(top_simulation_tables_rdw_opposite, 'rdw_opposite')\n",
    "\n",
    "# Dictionary to hold all top simulation tables\n",
    "tables_dict = {\n",
    "    'iid_same': top_simulation_tables_iid_same,\n",
    "    'iid_opposite': top_simulation_tables_iid_opposite,\n",
    "    'rdw_same': top_simulation_tables_rdw_same,\n",
    "    'rdw_opposite': top_simulation_tables_rdw_opposite\n",
    "}\n",
    "\n",
    "# Combine all into one large DataFrame\n",
    "all_combined_df = combine_simulation_tables(tables_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b907a-8bb3-4923-b970-c04e3d68f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 28\n",
    "top_simulation_tables_iid_same = get_top_simulations(simulation_tables_iid_same, mean_accuracy['iid_same'],28)\n",
    "top_simulation_tables_iid_opposite = get_top_simulations(simulation_tables_iid_opposite, mean_accuracy['iid_opposite'],28)\n",
    "top_simulation_tables_rdw_same = get_top_simulations(simulation_tables_rdw_same, mean_accuracy['rdw_same'],28)\n",
    "top_simulation_tables_rdw_opposite = get_top_simulations(simulation_tables_rdw_opposite, mean_accuracy['rdw_opposite'],28)\n",
    "\n",
    "# Save top simulation tables for each condition\n",
    "save_simulation_tables(top_simulation_tables_iid_same, 'iid_same','sim_28')\n",
    "save_simulation_tables(top_simulation_tables_iid_opposite, 'iid_opposite','sim_28')\n",
    "save_simulation_tables(top_simulation_tables_rdw_same, 'rdw_same','sim_28')\n",
    "save_simulation_tables(top_simulation_tables_rdw_opposite, 'rdw_opposite','sim_28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2a6ad-4942-423d-813d-df875e4d6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmdl = pd.DataFrame({\n",
    "    'rms_RSM_model_iid': rmdl[0, :],\n",
    "    'rms_RSM_model_rdw': rmdl[1, :],\n",
    "    'rms_LSTM_model_iid': rMLP[0, :],\n",
    "    'rms_LSTM_model_rdw': rMLP[1, :],\n",
    "    'meanRT_LSTM_model_iid': avg_rt_mlp[0, :],\n",
    "    'meanRT_LSTM_model_rdw': avg_rt_mlp[1, :],\n",
    "})\n",
    "\n",
    "df_rmdl.to_csv('rmse_sim_DUAl_LSTM_scaled_no_interp_full_sep_v0_100sim.csv', index=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc9453-1f26-4980-b5fa-2923e322c55a",
   "metadata": {},
   "source": [
    "## Check RT stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe73e46-5299-4ea2-8af6-c79f4e8df3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaad55d-c04f-40cf-91b2-4d603d71e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_type(file_path):\n",
    "    if \"iid_same\" in file_path:\n",
    "        return \"iid_same\"\n",
    "    elif \"iid_opposite\" in file_path:\n",
    "        return \"iid_opposite\"\n",
    "    elif \"rdw_same\" in file_path:\n",
    "        return \"rdw_same\"\n",
    "    elif \"rdw_opposite\" in file_path:\n",
    "        return \"rdw_opposite\"\n",
    "    else:\n",
    "        return \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d918ca-57d6-453b-a51c-d6dc9844a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv =  sorted(str(p) for p in pathlib.Path(\"./sim_100\").glob(\"*.csv\"))\n",
    "# data_csv = data_csv[0]\n",
    "sorted_files = sorted(data_csv, key=get_file_type)\n",
    "data_csv_iid_op = [file for file in sorted_files if get_file_type(file) == \"iid_opposite\"]\n",
    "data_csv_iid_same = [file for file in sorted_files if get_file_type(file) == \"iid_same\"]\n",
    "data_csv_rdw_op = [file for file in sorted_files if get_file_type(file) == \"rdw_opposite\"]\n",
    "data_csv_rdw_same = [file for file in sorted_files if get_file_type(file) == \"rdw_same\"]\n",
    "\n",
    "data_csv_iid_same  = sorted(data_csv_iid_same , key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "data_csv_iid_op  = sorted(data_csv_iid_op , key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "data_csv_rdw_op  = sorted(data_csv_rdw_op , key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "data_csv_rdw_same = sorted(data_csv_rdw_same , key=lambda x: int(x.split('_')[-1].split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3144b6-e8de-43f4-ae3e-c9c20f515fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_sim_iid_op = [pd.read_csv(file, header=0, sep='\\t') for file in data_csv_iid_op]\n",
    "data_sim_iid_same = [pd.read_csv(file, header=0, sep='\\t') for file in data_csv_iid_same]\n",
    "data_sim_rdw_op = [pd.read_csv(file, header=0, sep='\\t') for file in data_csv_rdw_op]\n",
    "data_sim_rdw_same = [pd.read_csv(file, header=0, sep='\\t') for file in data_csv_rdw_same]\n",
    "\n",
    "\n",
    "for i, df_list in enumerate(zip(data_sim_iid_op, data_sim_iid_same, data_sim_rdw_op, data_sim_rdw_same)):\n",
    "    for df in df_list:\n",
    "        df.insert(0, \"sim\", np.full(len(df), i, dtype=int), True)\n",
    "        df.insert(1, \"path\", range(0, len(df)), True)\n",
    "\n",
    "\n",
    "all_sim_iid_op = pd.concat(data_sim_iid_op, ignore_index=True)\n",
    "all_sim_iid_same = pd.concat(data_sim_iid_same, ignore_index=True)\n",
    "all_sim_rdw_op = pd.concat(data_sim_rdw_op, ignore_index=True)\n",
    "all_sim_rdw_same = pd.concat(data_sim_rdw_same, ignore_index=True)\n",
    "\n",
    "\n",
    "for df in [all_sim_iid_same, all_sim_iid_op, all_sim_rdw_same, all_sim_rdw_op]:\n",
    "    df['# rt'] = df['# rt'].round(4)\n",
    "\n",
    "\n",
    "def get_random_samples(data, n_samples=28):\n",
    "    selected_sims = np.random.choice(data['sim'].unique(), n_samples, replace=False)\n",
    "    return data[data['sim'].isin(selected_sims)]['# rt'].values\n",
    "\n",
    "#def get_random_samples(data, n_samples=28):\n",
    "#    unique_sims = data['sim'].unique()\n",
    "#    selected_sims = np.random.choice(unique_sims, n_samples, replace=False)\n",
    "#    sampled_data = data[data['sim'].isin(selected_sims)]\n",
    "#    return sampled_data['# rt'].values\n",
    "    \n",
    "rt_iid_same_sampled = get_random_samples(all_sim_iid_same)\n",
    "rt_iid_opp_sampled = get_random_samples(all_sim_iid_op)\n",
    "rt_rdw_same_sampled = get_random_samples(all_sim_rdw_same)\n",
    "rt_rdw_opp_sampled = get_random_samples(all_sim_rdw_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231b18d2-0b44-4b60-a3b7-bc43a9dda5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rdw = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/acc_rdw.csv', header=None).to_numpy()\n",
    "acc_iid = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/acc_iid.csv', header=None).to_numpy()\n",
    "\n",
    "\n",
    "rt_rdw = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/pros_rt_rdw.csv', header=None).to_numpy()\n",
    "rt_iid = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/pros_rt_iid.csv', header=None).to_numpy()\n",
    "\n",
    "\n",
    "acc_iid_same_path= np.round(np.vstack((acc_iid[0:50:,], acc_iid[150::,])).mean(axis=1),3)\n",
    "acc_iid_opp_path =np.round(acc_iid[50:150:,].mean(axis=1),3)\n",
    "acc_rdw_same_path =np.round(np.vstack((acc_rdw[0:50:,], acc_rdw[150::,])).mean(axis=1),3)\n",
    "acc_rdw_opp_path = np.round(acc_rdw[50:150:,].mean(axis=1),3)\n",
    "\n",
    "rt_iid_same_path= np.round(np.vstack((rt_iid[0:50:,], rt_iid[150::,])).mean(axis=1),3)\n",
    "rt_iid_opp_path =np.round(rt_iid[50:150:,].mean(axis=1),3)\n",
    "\n",
    "rt_rdw_same_path= np.round(np.vstack((rt_rdw[0:50:,], rt_rdw[150::,])).mean(axis=1),3)\n",
    "rt_rdw_opp_path = np.round(rt_rdw[50:150:,].mean(axis=1),3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc7b13-7a8f-4486-911d-df9ed7ac2288",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sim_df = pd.concat([all_sim_iid_op,all_sim_iid_same,all_sim_rdw_op,all_sim_rdw_same], ignore_index=True)\n",
    "all_sim_df_mean_path = all_sim_df.groupby(['path', 'condition'])[['# rt','acc']].mean().reset_index()\n",
    "all_sim_df_mean_path =all_sim_df_mean_path.sort_values(by=['condition','path'])\n",
    "all_sim_df_mean_path['acc_real']= np.hstack([acc_iid_opp_path,acc_iid_same_path,acc_rdw_opp_path,acc_rdw_same_path])\n",
    "all_sim_df_mean_path['rt_real']= np.hstack([rt_iid_opp_path,rt_iid_same_path,rt_rdw_opp_path,rt_rdw_same_path])\n",
    "\n",
    "all_sim_df_mean_path['# rt'] = all_sim_df_mean_path['# rt'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b1c08-8770-4ba9-8a9d-6e0d2b9c71e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Liste des conditions\n",
    "conditions = all_sim_df_mean_path['condition'].unique()\n",
    "\n",
    "# Créer des histogrammes avec KDE pour chaque condition\n",
    "for condition in conditions:\n",
    "    df_condition = all_sim_df_mean_path[all_sim_df_mean_path['condition'] == condition]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
    "    \n",
    "    # Histogramme et KDE pour acc vs acc_real\n",
    "    sns.histplot(df_condition['acc'], kde=True, color='blue', label='acc', ax=axes[0])\n",
    "    sns.histplot(df_condition['acc_real'], kde=True, color='orange', label='acc_real', ax=axes[0])\n",
    "    axes[0].set_title(f'Condition: {condition} - Acc')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Histogramme et KDE pour # rt vs rt_real\n",
    "    sns.histplot(df_condition['# rt'], kde=True, color='blue', label='# rt', ax=axes[1])\n",
    "    sns.histplot(df_condition['rt_real'], kde=True,color='orange', label='rt_real', ax=axes[1])\n",
    "    \n",
    "    axes[1].set_title(f'Condition: {condition} - RT')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xlim(0.2,0.6)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46730f30-3892-4be8-bfe8-7bd8cbd87b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rdw = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/acc_rdw.csv', header=None).to_numpy()\n",
    "acc_iid = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/acc_iid.csv', header=None).to_numpy()\n",
    "rt_rdw = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/pros_rt_rdw.csv', header=None).to_numpy()\n",
    "rt_iid = pd.read_csv('C:/Users/Oleg/Desktop/Paris_Saclay/These/4_DATA/data_exp_1/resp/Subjects_28_pross_rt/pros_rt_iid.csv', header=None).to_numpy()\n",
    "\n",
    "acc_iid_same= np.vstack((acc_iid[0:50:,], acc_iid[150::,])).flatten()\n",
    "acc_iid_opp =acc_iid[50:150:,].flatten() \n",
    "\n",
    "acc_rdw_same= np.vstack((acc_rdw[0:50:,], acc_rdw[150::,])).flatten()\n",
    "acc_rdw_opp = acc_rdw[50:150:,].flatten() \n",
    "\n",
    "rt_iid_same= np.vstack((rt_iid[0:50:,], rt_iid[150::,])).flatten()\n",
    "rt_iid_opp =rt_iid[50:150:,].flatten() \n",
    "\n",
    "rt_rdw_same = np.vstack((rt_rdw[0:50:,], rt_rdw[150::,])).flatten()\n",
    "rt_rdw_opp = rt_rdw[50:150:,].flatten() \n",
    "\n",
    "\n",
    "rt_rdw_same= rt_rdw_same[~np.isnan(rt_rdw_same)]\n",
    "rt_rdw_opp= rt_rdw_opp[~np.isnan(rt_rdw_opp)]\n",
    "\n",
    "rt_iid_same= rt_iid_opp[~np.isnan(rt_iid_opp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e779aea-9ed9-4133-947b-e36717e8237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_samples(data, column, n_samples=28):\n",
    "    #np.random.seed(42) \n",
    "    selected_sims = np.random.choice(data['sim'].unique(), n_samples, replace=False)\n",
    "    return data[data['sim'].isin(selected_sims)][column].values\n",
    "acc_iid_same_sampled = get_random_samples(all_sim_iid_same, 'acc')\n",
    "acc_iid_opp_sampled = get_random_samples(all_sim_iid_op, 'acc')\n",
    "acc_rdw_same_sampled = get_random_samples(all_sim_rdw_same, 'acc')\n",
    "acc_rdw_opp_sampled = get_random_samples(all_sim_rdw_op, 'acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c67f3-e614-4308-975e-ca91722068aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_samples(data, n_samples=28):\n",
    "    np.random.seed(20)\n",
    "    selected_sims = np.random.choice(data['sim'].unique(), n_samples, replace=False)\n",
    "    return data[data['sim'].isin(selected_sims)]['# rt'].values\n",
    "\n",
    "#def get_random_samples(data, n_samples=28):\n",
    "#    unique_sims = data['sim'].unique()\n",
    "#    selected_sims = np.random.choice(unique_sims, n_samples, replace=False)\n",
    "#    sampled_data = data[data['sim'].isin(selected_sims)]\n",
    "#    return sampled_data['# rt'].values\n",
    "    \n",
    "rt_iid_same_sampled = get_random_samples(all_sim_iid_same)\n",
    "rt_iid_opp_sampled = get_random_samples(all_sim_iid_op)\n",
    "rt_rdw_same_sampled = get_random_samples(all_sim_rdw_same)\n",
    "rt_rdw_opp_sampled = get_random_samples(all_sim_rdw_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d53106-3f5f-4c12-bcf5-2f53facfd4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "conditions = ['IID same', 'IID opposite', 'RDW same', 'RDW opposite']\n",
    "data_samples = [\n",
    "    (rt_iid_same_sampled, rt_iid_same),\n",
    "    (rt_iid_opp_sampled, rt_iid_opp),\n",
    "    (rt_rdw_same_sampled, rt_rdw_same),\n",
    "    (rt_rdw_opp_sampled, rt_rdw_opp)\n",
    "]\n",
    "\n",
    "for i, (sim_data, real_data) in enumerate(data_samples):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.distplot(sim_data, hist=True, kde=True, bins=10, color = 'blue', hist_kws={'edgecolor':'black'},\n",
    "                 label =f'Dual LSTM presicted')\n",
    "    sns.distplot(real_data, hist=True, kde=True, bins=60, color = 'orange', hist_kws={'edgecolor':'black'},\n",
    "                label =f'Real data')   # {conditions[i]}\n",
    "   \n",
    "    plt.xlabel('Reaction Time (s)', fontsize=16,fontname=\"Calibri\")\n",
    "    plt.ylabel('Frequency', fontsize=16,fontname=\"Calibri\")\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.title(conditions[i], fontsize=16,fontname=\"Calibri\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 4)\n",
    "    plt.xlim(-0.1, 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf2d83e-b2ea-4356-87c5-0fa5cb5f988f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50314f5c-5068-4dc7-b3b9-f2d82983baf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
