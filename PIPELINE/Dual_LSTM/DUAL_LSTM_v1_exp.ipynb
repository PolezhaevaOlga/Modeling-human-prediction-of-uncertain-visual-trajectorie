{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca8987e-b3f6-439d-aa5a-38a6f77c9c3a",
   "metadata": {},
   "source": [
    "# One model scaling all sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a996486-9609-42bc-aa48-0c81cf859522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import IPython\n",
    "import IPython.display\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import rcParams\n",
    "from keras.models import Sequential\n",
    "import scipy\n",
    "from scipy import io\n",
    "from scipy.interpolate import interp1d\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score,mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Flatten, Dense, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66cb24f-8db3-4205-a9e6-35c67b4e0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ok save mean and std, but min max\n",
    "def interpolate_signal(insignal, len1, len2):\n",
    "\n",
    "    first_point = insignal[0]\n",
    "    last_point = insignal[-1]\n",
    "\n",
    "    x1 = np.linspace(0, len2 - 1, len1)\n",
    "    f = interp1d(x1, insignal, axis=0, kind='linear')\n",
    "    x2 = np.linspace(0, len2 - 1, len2)\n",
    "    interpolated = f(x2)\n",
    "\n",
    "    interpolated[0] = first_point\n",
    "    interpolated[-1] = last_point\n",
    "    \n",
    "    return interpolated\n",
    "\n",
    "def list_to_interpolate_preserving_stats(input_list, output_size=145):\n",
    "    inter_list = []\n",
    "    for elem in input_list:\n",
    "        original_mean = np.nanmean(elem)\n",
    "        original_std = np.nanstd(elem)\n",
    "\n",
    "        np_int = interpolate_signal(elem, elem.shape[0], output_size)\n",
    "\n",
    "        central_part = np_int[1:-1]\n",
    "\n",
    "        interpolated_mean = np.nanmean(central_part)\n",
    "        interpolated_std = np.nanstd(central_part)\n",
    "\n",
    "        scaled_central = (central_part - interpolated_mean) * (original_std / interpolated_std) + original_mean\n",
    "\n",
    "        scaled_int = np.copy(np_int)\n",
    "        scaled_int[1:-1] = scaled_central\n",
    "\n",
    "        total_mean = np.nanmean(scaled_int)\n",
    "        total_std = np.nanstd(scaled_int)\n",
    "        scaled_int = (scaled_int - total_mean) * (original_std / total_std) + original_mean\n",
    "\n",
    "        scaled_int[0] = elem[0]\n",
    "        scaled_int[-1] = elem[-1]\n",
    "        inter_list.append(scaled_int)\n",
    "    \n",
    "    return inter_list\n",
    "\n",
    "def scaled_data2(input_list):\n",
    "\n",
    "    global_min = np.nanmin([np.nanmin(traj) for traj in input_list])\n",
    "    global_max = np.nanmax([np.nanmax(traj) for traj in input_list])\n",
    "\n",
    "    if global_max == global_min:\n",
    "        raise ValueError(\"All trajectories have the same constant value; scaling is not possible.\")\n",
    "    \n",
    "    normalized_list = []\n",
    "\n",
    "    for np_array in input_list:\n",
    "        output_array = 2 * ((np_array - global_min) / (global_max - global_min)) - 1\n",
    "\n",
    "        #output_array -= output_array[0]\n",
    "\n",
    "        #range_factor = max(1, np.max(np.abs(output_array)))\n",
    "        #output_array /= range_factor\n",
    "\n",
    "        normalized_list.append(output_array)\n",
    "\n",
    "    return normalized_list, global_max, global_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed40a90-4e35-45f2-be9b-07cc566b7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Load data\n",
    "    data = scipy.io.loadmat('../data/trajectories.mat')\n",
    "    \n",
    "    # Path\n",
    "    iid = data['iidind'][2:,]\n",
    "    rdw = data['rdwind'][2:,]\n",
    "    posr = np.zeros((iid.shape[0] - 1, iid.shape[1]))\n",
    "    posw = np.zeros((rdw.shape[0]-1, rdw.shape[1]))\n",
    "    n_path = posr.shape[1]\n",
    "    invis = []\n",
    "    normativ_label_iid = np.zeros((1, n_path))  # ideal responses\n",
    "    normativ_label_rdw = np.zeros((1, n_path))  # ideal responses\n",
    "    for tidx in range(n_path):\n",
    "        # IID\n",
    "        invisR = int(iid[0, tidx])\n",
    "        invis.append(invisR)\n",
    "        r = iid[1:, tidx].copy()\n",
    "        #r[-invisR + 1:] = np.nan\n",
    "        r[-invisR + 1:] = 0\n",
    "        posr[:, tidx] = r\n",
    "        normativ_label_iid[0, tidx] = 1 if np.nanmean(posr[:, tidx]) < 0 else 0\n",
    "        # rdw \n",
    "        w = rdw[1:, tidx].copy()\n",
    "        #w[-invisR+1:] = np.nan\n",
    "        w[-invisR+1:] = 0\n",
    "        posw[:, tidx] = w\n",
    "        normativ_label_rdw[0, tidx] = 1 if posw[-invis[tidx], tidx] < 0 else 0\n",
    "    \n",
    "    # # Create a list of columns from posr without NaN values\n",
    "    # path_iid = [np.array(col[~np.isnan(col)]) for col in posr.T]\n",
    "    \n",
    "    # # Create a list of columns from posw without NaN values\n",
    "    # path_rdw = [np.array(col[~np.isnan(col)]) for col in posw.T]\n",
    "    normativ_label_rdw =normativ_label_rdw.reshape(200,)\n",
    "    normativ_label_iid =normativ_label_iid.reshape(200,)\n",
    "    \n",
    "    ### rdw ###\n",
    "    normativ_label_rdw_same = np.hstack((normativ_label_rdw[0:50], normativ_label_rdw[150:]))\n",
    "    normativ_label_rdw_op = normativ_label_rdw[50:150]\n",
    "    normativ_list_label_rdw_same = [np.array(col[~np.isnan(col)]) for col in normativ_label_rdw_same]\n",
    "    normativ_list_label_rdw_op = [np.array(col[~np.isnan(col)]) for col in normativ_label_rdw_op]\n",
    "    normativ_labels_r = np.concatenate((normativ_list_label_rdw_same, normativ_list_label_rdw_op ), axis=0)\n",
    "\n",
    "    posw_same = np.hstack((posw[:,0:50], posw[:,150:]))\n",
    "    posw_opposite = posw[:,50:150]\n",
    "    # Create a list of columns from posr without NaN values\n",
    "    path_rdw_s = [np.array(col[~np.isnan(col)]) for col in posw_same.T]\n",
    "    path_rdw_op =[np.array(col[~np.isnan(col)]) for col in posw_opposite.T]\n",
    "\n",
    "    ### iid ###\n",
    "    normativ_label_iid_same = np.hstack((normativ_label_iid[0:50], normativ_label_iid[150:]))\n",
    "    normativ_label_iid_op = normativ_label_iid[50:150]\n",
    "    normativ_list_label_iid_same = [np.array(col[~np.isnan(col)]) for col in normativ_label_iid_same]\n",
    "    normativ_list_label_iid_op = [np.array(col[~np.isnan(col)]) for col in normativ_label_iid_op]\n",
    "    normativ_labels_i = np.concatenate((normativ_list_label_iid_same, normativ_list_label_iid_op ), axis=0)\n",
    "\n",
    "    posr_same = np.hstack((posr[:,0:50], posr[:,150:]))\n",
    "    posr_opposite = posr[:,50:150]\n",
    "    # Create a list of columns from posr without NaN values\n",
    "    path_iid_s = [np.array(col[~np.isnan(col)]) for col in posr_same.T]\n",
    "    path_iid_op =[np.array(col[~np.isnan(col)]) for col in posr_opposite.T]    \n",
    "    \n",
    "    return path_iid_s, path_iid_op, path_rdw_s, path_rdw_op\n",
    "\n",
    "\n",
    "def interpolate_signal(insignal, len1, len2):\n",
    "\n",
    "    first_point = insignal[0]\n",
    "    last_point = insignal[-1]\n",
    "\n",
    "    x1 = np.linspace(0, len2 - 1, len1)\n",
    "    f = interp1d(x1, insignal, axis=0, kind='linear')\n",
    "    x2 = np.linspace(0, len2 - 1, len2)\n",
    "    interpolated = f(x2)\n",
    "\n",
    "    interpolated[0] = first_point\n",
    "    interpolated[-1] = last_point\n",
    "    \n",
    "    return interpolated\n",
    "\n",
    "def list_to_interpolate_preserving_stats(input_list, output_size=145):\n",
    "    inter_list = []\n",
    "    for elem in input_list:\n",
    "        np_int = interpolate_signal(elem, elem.shape[0], output_size)\n",
    "        inter_list.append(np_int)\n",
    "    return inter_list\n",
    "\n",
    "def scaled_data2(input_list):\n",
    "\n",
    "    global_min = np.nanmin([np.nanmin(traj) for traj in input_list])\n",
    "    global_max = np.nanmax([np.nanmax(traj) for traj in input_list])\n",
    "\n",
    "    if global_max == global_min:\n",
    "        raise ValueError(\"All trajectories have the same constant value; scaling is not possible.\")\n",
    "    \n",
    "    normalized_list = []\n",
    "\n",
    "    for np_array in input_list:\n",
    "        output_array = 2 * ((np_array - global_min) / (global_max - global_min)) - 1\n",
    "\n",
    "        #output_array -= output_array[0]\n",
    "\n",
    "        #range_factor = max(1, np.max(np.abs(output_array)))\n",
    "        #output_array /= range_factor\n",
    "\n",
    "        normalized_list.append(output_array)\n",
    "\n",
    "    return normalized_list, global_max, global_min\n",
    "\n",
    "def load_responses(resp_array):\n",
    "    resp_same =  np.vstack((resp_array[0:50:,], resp_array[150::,]))\n",
    "    resp_op = resp_array[50:150:,]\n",
    "    return resp_same,resp_op\n",
    "\n",
    "\n",
    "def shuffle_data(trajectories, responses,rt):\n",
    "    indices = np.arange(len(trajectories))  \n",
    "    np.random.seed(15)\n",
    "    np.random.shuffle(indices)  \n",
    "    \n",
    "    shuffled_trajectories = [trajectories[i] for i in indices]  \n",
    "    shuffled_responses = responses[indices,: ]  \n",
    "    shuffled_rt = rt[indices,: ]      \n",
    "    return shuffled_trajectories, shuffled_responses,shuffled_rt \n",
    "\n",
    "def make_dataset (path_iid_s,path_iid_op, resp_iid_s,resp_iid_op,rt_iid_s,rt_iid_op):\n",
    "    feature_list =[]\n",
    "    label_list = []\n",
    "    rt_list=[]\n",
    "    for i in range(resp_iid_s.shape[1]):\n",
    "        resp_one_subj_iid_s = resp_iid_s[:, i]\n",
    "        resp_one_subj_iid_op = resp_iid_op[:, i]\n",
    "        \n",
    "        rt_one_subj_iid_s = rt_iid_s[:, i]\n",
    "        rt_one_subj_iid_op = rt_iid_op[:, i]  \n",
    "        \n",
    "        resp_list_iid_s = [np.array(col[~np.isnan(col)]) for col in resp_one_subj_iid_s]\n",
    "        resp_list_iid_op = [np.array(col[~np.isnan(col)]) for col in resp_one_subj_iid_op]\n",
    "\n",
    "        rt_list_iid_s = [col[col != 0] for col in rt_one_subj_iid_s]\n",
    "        rt_list_iid_op = [col[col != 0] for col in rt_one_subj_iid_op]\n",
    "\n",
    "        scaled_data = np.concatenate((path_iid_s, path_iid_op), axis=0)\n",
    "        labels = np.concatenate((resp_list_iid_s, resp_list_iid_op), axis=0)\n",
    "       \n",
    "        rts = np.concatenate((rt_list_iid_s, rt_list_iid_op), axis=0)\n",
    "\n",
    "        feature_list.append(scaled_data)\n",
    "        label_list.append(labels)\n",
    "        rt_list.append(rts)\n",
    "    return feature_list, label_list,rt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90382b99-41d6-4aa2-924d-0b0f8ac90534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(feature_list, label_list,rt_list, condition):\n",
    "    combined_data = list(zip(feature_list, label_list, rt_list)) # Combine features and labels into a single list for shuffling\n",
    "    np.random.seed(10)\n",
    "    np.random.shuffle(combined_data) # Shuffle the combined data\n",
    "    feature_list, label_list, rt_list = zip(*combined_data) # Unpack the shuffled data back into features and labels\n",
    "    feature_list = list(feature_list) # Convert the shuffled lists back to regular Python lists\n",
    "    label_list = list(label_list)\n",
    "    rt_list = list(rt_list)\n",
    "    \n",
    "    norm_list= []\n",
    "    # norm rdw \n",
    "    if condition==\"rdw\":\n",
    "        norm_list.extend([np.where(feature_list[0][:,-1] > 0, 0, 1)] * 28)    \n",
    "    elif condition==\"iid\":\n",
    "    #norm iid \n",
    "        norm_list.extend([np.where(np.mean(feature_list[0].T, axis=0) > 0, 0, 1)] * 28)\n",
    "    \n",
    "    X = np.concatenate([f.reshape(f.shape[0], -1) for f in feature_list], axis=0)    \n",
    "    y = np.concatenate(label_list, axis=0)\n",
    "    rt = np.concatenate(rt_list, axis=0)\n",
    "    \n",
    "    norm = np.array(norm_list)\n",
    "    all_y = np.concatenate([y[i].reshape(-1) for i in range(len(y))], axis=0)\n",
    "    all_rt = np.concatenate([rt[i].reshape(-1) for i in range(len(rt))], axis=0)\n",
    "    all_norm = np.concatenate([norm[i].reshape(-1) for i in range(len(norm))], axis=0)\n",
    "    # X.shape, y.shape,all_y.shape, all_norm.shape\n",
    "    return X,all_y,all_rt,all_norm\n",
    "\n",
    "\n",
    "def remove_nan_indices(X, y, rt):\n",
    "   \n",
    "    #X = np.array(X)\n",
    "    #y = np.array(y)\n",
    "    #rt = np.array(rt)\n",
    "\n",
    "    valid_indices = ~np.isnan(rt)\n",
    "    X_clean = X[valid_indices]\n",
    "    y_clean = y[valid_indices]\n",
    "    rt_clean = rt[valid_indices]\n",
    "\n",
    "    return X_clean, y_clean, rt_clean\n",
    "\n",
    "def create_recent_points_data(X, y, rt, window_size):\n",
    "    X_windows = []\n",
    "    y_windows = []\n",
    "    rt_windows = []\n",
    "    \n",
    "    for i in range(X.shape[0]):  \n",
    "        trajectory = X[i]\n",
    "        label = y[i]\n",
    "        rt_value = rt[i]\n",
    "        invis = len(np.where(trajectory == 0)[0])-1\n",
    "        trajectory_non_zero = trajectory[:-invis]\n",
    "        # last point in window_size \n",
    "        window = trajectory_non_zero[-window_size:]\n",
    "        X_windows.append(window)\n",
    "        y_windows.append(label)\n",
    "        rt_windows.append(rt_value)\n",
    "    \n",
    "    return np.array(X_windows), np.array(y_windows), np.array(rt_windows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08137093-b735-48fe-b75b-88e69d44e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_iid_s, path_iid_op, path_rdw_s, path_rdw_op = load_data()\n",
    "\n",
    "\n",
    "rt_rdw = pd.read_csv('../data/pros_rt_rdw.csv', header=None).to_numpy()\n",
    "rt_iid = pd.read_csv('../data/pros_rt_iid.csv', header=None).to_numpy()\n",
    "resp_rdw = pd.read_csv('../data/resp_rdw.csv', header=None).to_numpy()\n",
    "resp_iid = pd.read_csv('../data/resp_iid.csv', header=None).to_numpy()\n",
    "\n",
    "acc_rdw = pd.read_csv('../data/acc_rdw.csv', header=None).to_numpy()\n",
    "acc_iid = pd.read_csv('../data/acc_iid.csv', header=None).to_numpy()\n",
    "\n",
    "resp_iid_same,resp_iid_op= load_responses(resp_iid)\n",
    "resp_rdw_same,resp_rdw_op= load_responses(resp_rdw)\n",
    "\n",
    "rt_iid_same,rt_iid_op= load_responses(rt_iid)\n",
    "rt_rdw_same,rt_rdw_op= load_responses(rt_rdw)\n",
    "\n",
    "norm_acc_iid_same,norm_acc_iid_op = load_responses(acc_iid)\n",
    "norm_acc_rdw_same,norm_acc_rdw_op = load_responses(acc_rdw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c9d6fe-f69d-4c1d-9550-bdeb652a9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### IID ##########\n",
    "##interpolation \n",
    "path_iid_same_interp_list = list_to_interpolate_preserving_stats(path_iid_s, output_size=150)\n",
    "path_iid_op_interp_list = list_to_interpolate_preserving_stats(path_iid_op, output_size=150)\n",
    "# Normalization\n",
    "scaled_list_iid_same, _, _,  = scaled_data2(path_iid_same_interp_list)  # Transpose for correct shape\n",
    "scaled_list_iid_op, _, _,  = scaled_data2(path_iid_op_interp_list)  # Transpose for correct shape\n",
    "\n",
    "######### RDW ##########\n",
    "##interpolation \n",
    "path_rdw_same_interp_list = list_to_interpolate_preserving_stats(path_rdw_s, output_size=150)\n",
    "path_rdw_op_interp_list = list_to_interpolate_preserving_stats(path_rdw_op, output_size=150)\n",
    "## Normalization\n",
    "scaled_list_rdw_same, _, _,  = scaled_data2(path_rdw_same_interp_list)  # Transpose for correct shape\n",
    "scaled_list_rdw_op, _, _,  = scaled_data2(path_rdw_op_interp_list)  # Transpose for correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443098d3-15ce-4b0d-ad50-5e7186e7932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_list_iid_same, resp_iid_same,rt_iid_same = shuffle_data(scaled_list_iid_same, resp_iid_same,rt_iid_same)\n",
    "scaled_list_iid_op, resp_iid_op, rt_iid_op = shuffle_data(scaled_list_iid_op, resp_iid_op,rt_iid_op)\n",
    "\n",
    "scaled_list_rdw_same, resp_rdw_same,rt_rdw_same = shuffle_data(scaled_list_rdw_same, resp_rdw_same,rt_rdw_same)\n",
    "scaled_list_rdw_op, resp_rdw_op,rt_rdw_op = shuffle_data(scaled_list_rdw_op, resp_rdw_op,rt_rdw_op)\n",
    "\n",
    "feature_list_iid, label_list_iid,rt_list_iid = make_dataset(scaled_list_iid_same,scaled_list_iid_op, \n",
    "                                                        resp_iid_same,resp_iid_op,rt_iid_same, rt_iid_op)\n",
    "\n",
    "feature_list_rdw, label_list_rdw,rt_list_rdw = make_dataset (scaled_list_rdw_same,scaled_list_rdw_op,\n",
    "                                                             resp_rdw_same,resp_rdw_op,rt_rdw_same, rt_rdw_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba82b1-062e-4df0-bfd7-3e79cc4e5b91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(scaled_list_rdw_same)):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    invis = len(np.where(scaled_list_iid_same[i] == 0)[0])\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(scaled_list_iid_same[i],label=f\"path iid {i}, (mu: {round(np.nanmean(scaled_list_iid_same[i]),3)}, sd: {round(np.nanstd(scaled_list_iid_same[i]),3)} )\")#\n",
    "    plt.plot(scaled_list_rdw_same[i],label=f\"path rdw {i}, (mu: {round(np.nanmean(scaled_list_rdw_same[i]),3)}, sd: {round(np.nanstd(scaled_list_rdw_same[i]),3)} )\")\n",
    "    plt.plot(len(scaled_list_iid_same[i])-invis, scaled_list_iid_same[i][-invis] ,'o',label=f\"last iid {round(scaled_list_iid_same[i][-invis],2)}\")\n",
    "    plt.plot(len(scaled_list_rdw_same[i])-invis, scaled_list_rdw_same[i][-invis] ,'o',label=f\"last rdw {round(scaled_list_rdw_same[i][-invis],2)}\")\n",
    "    plt.hlines(0, 0, len(scaled_list_rdw_same[i]),'k',  alpha=0.2)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    #     plt.title(f\"IID path ({mean_side}{lv}{cond})\")\n",
    "#     plt.ylim(-0, 1)\n",
    "    plt.xlim(0, 150)\n",
    "    plt.ylabel(\"X position\", fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    plt.title(f\"path {i}  \")\n",
    "#     plt.ylim(-0, 1)\n",
    "    plt.xlim(0, 150)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.ylabel(\"X position\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f1bac-73a3-4127-8d4e-af0a62adf403",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iid, all_y_iid, all_rt_iid,all_norm_iid = shuffle_dataset(feature_list_iid, label_list_iid,rt_list_iid,\"iid\")\n",
    "X_rdw, all_y_rdw,all_rt_rdw, all_norm_rdw =shuffle_dataset(feature_list_rdw, label_list_rdw,rt_list_rdw,\"rdw\")\n",
    "\n",
    "X_iid, all_y_iid, all_rt_iid = remove_nan_indices(X_iid, all_y_iid, all_rt_iid)\n",
    "X_rdw, all_y_rdw, all_rt_rdw = remove_nan_indices(X_rdw, all_y_rdw, all_rt_rdw)\n",
    "\n",
    "X_iid_reshaped = X_iid.reshape(X_iid.shape[0], X_iid.shape[1], 1)\n",
    "X_rdw_reshaped = X_rdw.reshape(X_rdw.shape[0], X_rdw.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e714c72e-ab6d-42d3-a387-b065f6959e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(model, X_full_trajectory, X_recent_trajectory, y_true_binary, y_true_time):\n",
    "\n",
    "    predictions = model.predict({'full_trajectory': X_full_trajectory, 'recent_trajectory': X_recent_trajectory})\n",
    "\n",
    "    binary_predictions = predictions[0]  # Binary response predictions\n",
    "    time_predictions = predictions[1]    # Reaction time predictions\n",
    "\n",
    "    binary_predictions = (binary_predictions > 0.5).astype(int)\n",
    "\n",
    "    binary_accuracy = accuracy_score(y_true_binary, binary_predictions)\n",
    "    precision = precision_score(y_true_binary, binary_predictions)\n",
    "    recall = recall_score(y_true_binary, binary_predictions)\n",
    "    f1 = f1_score(y_true_binary, binary_predictions)\n",
    "    conf_matrix = confusion_matrix(y_true_binary, binary_predictions)\n",
    "\n",
    "\n",
    "    rmse_time = np.sqrt(mean_squared_error(y_true_time, time_predictions))\n",
    "    relative_rmse_time = rmse_time / (all_rt_iid.max() - all_rt_iid.min())\n",
    "    metrics = {\n",
    "        'Binary Accuracy': binary_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'RMSE for RT Predictions': rmse_time,\n",
    "        'Relative RMSE for RT Predictions': relative_rmse_time\n",
    "    }\n",
    "\n",
    "    return metrics,predictions\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "\n",
    "    rcParams.update({'font.size': 20})  \n",
    "    rcParams['font.family'] = 'sans-serif'\n",
    "    rcParams['font.sans-serif'] = ['Calibri']  \n",
    "    \n",
    "    # График точности\n",
    "    plt.figure(figsize=(27, 9))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['response_accuracy'])\n",
    "    plt.plot(history.history['val_response_accuracy'])\n",
    "    plt.title('Model Accuracy (Response)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # График ошибки (Loss)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history.history['reaction_time_mse'])\n",
    "    plt.plot(history.history['val_reaction_time_mse'])\n",
    "    plt.title('Mean Squared Error (RT)')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ffe029-0a18-44ed-aaf3-36b20e11f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, X_full_train, X_recent_train, y_binary_train, y_time_train, \n",
    "                    X_full_val, X_recent_val, y_binary_val, y_time_val, patience=40):\n",
    " \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "    \n",
    "  \n",
    "    history = model.fit(\n",
    "        {'full_trajectory': X_full_train, 'recent_trajectory': X_recent_train}, \n",
    "        {'response': y_binary_train, 'reaction_time': y_time_train},  \n",
    "        epochs=100, batch_size=64, \n",
    "        validation_data=(\n",
    "            {'full_trajectory': X_full_val, 'recent_trajectory': X_recent_val},  \n",
    "            {'response': y_binary_val, 'reaction_time': y_time_val} \n",
    "        ),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aee729-999d-4e10-890c-f54808b55daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Flatten, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "best_window = None\n",
    "best_score = float('inf')\n",
    "metrics_table = []\n",
    "results = {}\n",
    "\n",
    "for window_size in range(1, 21):\n",
    "    print(f\"\\nTraining with window_size = {window_size}\")\n",
    "\n",
    "    # Обновляем данные с учетом нового окна\n",
    "    X_iid_recent, y_iid_recent, rt_iid_recent = create_recent_points_data(X_iid_reshaped, all_y_iid, all_rt_iid, window_size)\n",
    "    X_rdw_recent, y_rdw_recent, rt_rdw_recent = create_recent_points_data(X_rdw_reshaped, all_y_rdw, all_rt_rdw, window_size)\n",
    "\n",
    "    X_combined_recent = np.concatenate((X_iid_recent, X_rdw_recent), axis=0)\n",
    "    y_combined_recent = np.concatenate((y_iid_recent, y_rdw_recent), axis=0)\n",
    "    rt_combined = np.concatenate((rt_iid_recent, rt_rdw_recent), axis=0)\n",
    "    X_combined_full = np.concatenate((X_iid_reshaped, X_rdw_reshaped), axis=0)\n",
    "\n",
    "    # Разделение на обучение/валидацию\n",
    "    X_full_train, X_full_val, X_recent_train, X_recent_val, y_binary_train, y_binary_val, y_time_train, y_time_val = train_test_split(\n",
    "        X_combined_full, X_combined_recent, y_combined_recent, rt_combined, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Новая архитектура модели\n",
    "    full_trajectory_input = Input(shape=(150, 1), name='full_trajectory')\n",
    "    recent_trajectory_input = Input(shape=(window_size, 1), name='recent_trajectory')\n",
    "\n",
    "    full_lstm_layer = LSTM(64, return_sequences=False)(full_trajectory_input)\n",
    "    full_lstm_layer = Dropout(0.2)(full_lstm_layer)\n",
    "\n",
    "    recent_lstm_layer = LSTM(32, return_sequences=False)(recent_trajectory_input)\n",
    "    recent_lstm_layer = Dropout(0.2)(recent_lstm_layer)\n",
    "\n",
    "    combined_layer = Concatenate()([full_lstm_layer, recent_lstm_layer])\n",
    "    mlp_input = Flatten()(combined_layer)\n",
    "\n",
    "    dense_layer_1 = Dense(32, activation='relu')(mlp_input)\n",
    "    dense_layer_2 = Dense(16, activation='relu')(dense_layer_1)\n",
    "    time_output = Dense(1, name='reaction_time', activation='linear')(dense_layer_2)\n",
    "    response_output = Dense(1, activation='sigmoid', name='response')(mlp_input)\n",
    "\n",
    "    model = Model(inputs=[full_trajectory_input, recent_trajectory_input],\n",
    "                  outputs=[response_output, time_output])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss={'response': 'binary_crossentropy', 'reaction_time': 'mse'},\n",
    "                  metrics={'response': 'accuracy', 'reaction_time': 'mse'})\n",
    "\n",
    "    print(f\"\\nStart Training for window_size = {window_size}\")\n",
    "    history = model.fit(\n",
    "        {'full_trajectory': X_full_train, 'recent_trajectory': X_recent_train},\n",
    "        {'response': y_binary_train, 'reaction_time': y_time_train},\n",
    "        validation_data=(\n",
    "            {'full_trajectory': X_full_val, 'recent_trajectory': X_recent_val},\n",
    "            {'response': y_binary_val, 'reaction_time': y_time_val}\n",
    "        ),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "        verbose=0\n",
    "    )\n",
    "    plot_training_history(history)\n",
    "    # Метрики\n",
    "    train_loss = history.history['loss'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    train_acc = history.history['response_accuracy'][-1]\n",
    "    val_acc = history.history['val_response_accuracy'][-1]\n",
    "    train_rmse = np.sqrt(history.history['reaction_time_mse'][-1])\n",
    "    val_rmse = np.sqrt(history.history['val_reaction_time_mse'][-1])\n",
    "    overfit_acc = abs(train_acc - val_acc)\n",
    "    overfit_rmse = abs(train_rmse - val_rmse)\n",
    "\n",
    "    score = val_rmse + (1 - val_acc) + 0.5 * (overfit_acc + overfit_rmse)\n",
    "\n",
    "    metrics_table.append({\n",
    "        'window_size': window_size,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'train_accuracy': train_acc,\n",
    "        'val_accuracy': val_acc,\n",
    "        'train_rmse': train_rmse,\n",
    "        'val_rmse': val_rmse,\n",
    "        'overfit_acc': overfit_acc,\n",
    "        'overfit_rmse': overfit_rmse,\n",
    "        'score': score\n",
    "    })\n",
    "\n",
    "    results[window_size] = score\n",
    "\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_window = window_size\n",
    "        print(f\"New best window_size: {best_window} with score = {best_score:.4f}\")\n",
    "\n",
    "    K.clear_session()\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820204bb-c6de-4c27-9662-d2b436356d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.DataFrame(metrics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50314f5c-5068-4dc7-b3b9-f2d82983baf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
